[["index.html", "Composite Variables 1 Preface", " Composite Variables Jon Lefcheck January 16, 2021 1 Preface Structural equation modeling is among the fastest growing statistical techniques in the natural sciences, thanks in large part to new advances and software packages that make it broadly applicable and easy to use. This book is meant to be an approachable and open-source guide to the theory, math, and application of SEM. It integrates code for the R software for statistical computing from popular packages such as lavaan and piecewiseSEM. Each chapter ends with worked examples from the published literature. Moreover, as the author of the piecewiseSEM package, this format allows me to document newly-deployed functionality in the package, such as the addition of categorical variables, multigroup analysis and composite variables, new forms of coefficient standardization, and updates to model R2s. Check back often, as this book is a living resource: as new functionality is added and bugs uncovered and fixed, they will be described in detail here (with worked examples where possible). I would also say that this book is not a peer-reviewed resource, and has been somewhat cobbled together over late night coding sessions prepping for morning lectures. So please take everything here with a grain of salt, and do not hesitate to reach out if you find an errors, typos, or other issues. I am, however, indebted to Christopher Shannon, Audrey Barker-Plotkin, Scott Meyers, Liangzhe Chen, and others for their careful reading and for providing excellent editorial and technical corrections. Happy coding! "],["global-estimation.html", "2 Global Estimation 2.1 What is (Co)variance? 2.2 Regression Coefficients 2.3 Variance-based Structural Equation Modeling 2.4 Model Identifiability 2.5 Goodness-of-fit Measures 2.6 Model Fitting Using lavaan 2.7 References", " 2 Global Estimation 2.1 What is (Co)variance? The building block of the global estimation procedure for SEM is variance, specifically the covariance between variables. Before we delve into the specifics of this procedure, its worth reviewing the basics of variance and covariance. Variance is the degree of spread in a set of data. Formally, it captures the deviation of each point from the mean value across all points. Consider the variable \\(x\\). The variance of \\(x\\) is calculated as: \\[VAR_{x} = \\frac{\\sum(x_{i} - \\overline{x})^2}{n-1}\\] where \\(x_{i}\\) is each sample value, \\(\\overline{x}\\) is the sample mean, and \\(n\\) is the sample size. Similarly, for the response \\(y\\): \\[VAR_{y} = \\frac{\\sum(y_{i} - \\overline{y})^2}{n-1}\\] Note that, regardless of the actual scale of the variables, variances are always positive (due to the squared term). The larger the variance, the more spread out the data are around the mean. Covariance is a measure of the dependency between two variables. Covariance can be formalized as: \\[COV_{xy} = \\frac{\\sum(x_{i} - \\overline{x}) (y_{i} - \\overline{y})}{n - 1}\\] If variation in \\(x\\) tends to track variation in \\(y\\), then the numerator is large and covariance is high. In this case, the two variables are then said to co-vary. Consider a simple example. In R, the function var computes variance, and cov the covariance, which we can compare to coding the equations above by hand: x &lt;- c(1, 2, 3, 4) y &lt;- c(2, 3, 4, 5) # variance in x sum((x - mean(x))^2)/(length(x) - 1) == var(x) ## [1] TRUE # variance in y sum((y - mean(y))^2)/(length(y) - 1) == var(y) ## [1] TRUE # covariance sum((x - mean(x)) * (y - mean(y)))/(length(x) - 1) == cov(x, y) ## [1] TRUE The variance and covariance depend on the magnitude of the units. If the units of \\(x\\) are much larger than \\(y\\), then the covariance will also be large: cov(x, y) ## [1] 1.666667 # increase units of x x1 &lt;- x * 1000 cov(x1, y) ## [1] 1666.667 This property can make the interpretation and comparison of (co)variances among sets of variables misleading if the units are very different between variables. To solve this issue, we can standardize the variables to have a mean of 0 and a variance of 1. This standardization is achieved by subtracting the mean from each observation, and dividing by the standard deviation of the mean (the square-root of the variance). This procedure is also known as the Z-transformation. zx &lt;- (x - mean(x)) / sd(x) zy &lt;- (y - mean(y)) / sd(y) # can also be obtained using the function `?scale` Replacing the values of x and y with the standardized versions in our calculation of covariance yields the Pearson product moment correlation, \\(r\\). Correlations are in units of standard deviations of the mean, and thus can be fairly compared regardless of the magnitude of the original variables. The function to compute the correlation is cor: sum(zx * zy)/(length(zx) - 1) == cor(x, y) ## [1] TRUE In our example, the two variables are perfectly correlated, so \\(r = 1\\). Incidentally, this is the same as dividing the covariance of \\(x\\) and \\(y\\) by the product of their standard deviations, which omits the need for the prior Z-transformation step but achieves the same outcome: (cov(x, y) / (sd(x) * sd(y))) == cor(x, y) ## [1] FALSE Now that we have reviewed these basic concepts, we can begin to consider them within the context of SEM. 2.2 Regression Coefficients The inferential heart of structural equation modeling are the regression (or path) coefficients. These values mathematically quantify the (mostly linear) dependence of one variable on another. This verbiage should sound familiar because that is what we have already established as the goal of covariance and correlation. In this section, we will demonstrate how path coefficients can be derived from correlation coefficients and explore Graces 8 rules of path coefficients. First, we must define the important distinction between a regression (path) coefficient and a correlation coefficient. In a simple linear regression, one variable \\(y\\) is the response and another \\(x\\) is the predictor. The association between the two variables is used to generate the predictor \\(\\hat{y}\\): \\[\\hat{y} = bx + a\\] where \\(b\\) is the regression coefficient and \\(a\\) is the intercept. Its important to note that \\(b\\) implies a linear relationship, i.e., the relationship between \\(x\\) and \\(y\\) can be captured by a straight line (for now, as we will see later there are ways to relax this restriction). The regression coefficient between \\(x\\) and \\(y\\) can be related to the correlation coefficient through the following equation: \\[b_{xy} = r_{xy} (SD_{y}/SD_{x})\\] If the variables have been Z-transformed, then the \\(SD_{x} = SD_{y} = 1\\) and \\(b_{xy} = r_{xy}\\). This brings us to our first key point: when the variables have been scaled to mean = 0 and variance = 1, then the regression coefficient is the correlation coefficient. For multiple regression (more than one predictor \\(x1\\), \\(x2\\), and so on), these values are the partial correlation coefficients. We refer to scaled relationships as standardized coefficients. Unstandardized coefficients, in contrast, are reported in their raw units. As with variance, their values depend on the unit of measure. In fact, the unstandardized coefficient can be related to the variance through the following equation, which may appear familiar, as this is how we achieved a correlation coefficient from covariances earlier: \\[b_{xy} = \\frac{COV_{xy}}{VAR_{x}}\\] In mathematical terms, the unstandardized coefficients are scaled by the variance of the predictor, while the standardized variance by the ratio of the standard deviations of both \\(x\\) and \\(y\\). We can demonstrate these principles using a simple example: set.seed(111) data &lt;- data.frame(y1 = runif(100)) data$x1 &lt;- data$y1 + runif(100) unstd.model &lt;- lm(y1 ~ x1, data) # get unstandardized coefficient summary(unstd.model)$coefficients[2, 1] ## [1] 0.462616 # now using covariance (cov(data$y1, data$x1) / var(data$x)) ## [1] 0.462616 # repeat with scaled data std.model &lt;- lm(y1 ~ x1, as.data.frame(apply(data, 2, scale))) # get standardized coefficient summary(std.model)$coefficients[2, 1] ## [1] 0.6964617 # now using correlation cor(data$y1, data$x1) ## [1] 0.6964617 The concepts of variance, covariance, and correlation therefore directly inform the calculation of unstandardized and standardized regression coefficients and lend them their unique properties that we will now cover in the 8 rules of path coefficients. 2.2.1 Rule 1: Unspecified relationships among exogenous variables are simply their bivariate correlations. Variables that only have paths emanating from them (i.e., do not have arrows going into them) are called exogenous variables. If there is not a directed path between two exogenous variables, then their relationship can be expressed by the simple correlation between them. This is sometimes, but not always, indicated by a double-headed arrow. So \\(x1 &lt;-&gt; x2 == cor(x1, x2)\\). In contrast, variables for which arrows are also directed into are called endogenous. An endogenous variable can also have arrows directing out of it, but the sole condition is that they must be predicted (rather than solely predictors). 2.2.2 Rule 2: When two variables are connected by a single path, the coefficient of that path is the regression coefficient. For this rule, we will expand upon our earlier example to construct a simple path diagram: data$y2 &lt;- data$y1 + runif(100) In this case, the path coefficient connecting \\(x1 -&gt; y1\\) is the regression coefficient \\(\\gamma_{y1,x1}\\). Similarly, the path coefficient connecting \\(y1 -&gt; y2\\) is the regression coefficient \\(\\beta_{y2,y1}\\). If the data are standardized, then the regression coefficient equals the correlation between the two: (pathx1_y1 &lt;- summary(lm(y1 ~ x1, as.data.frame(apply(data, 2, scale))))$coefficients[2, 1]) ## [1] 0.6964617 cor(data$y1, data$x1) ## [1] 0.6964617 (pathy1_y2 &lt;- summary(lm(y2 ~ y1, as.data.frame(apply(data, 2, scale))))$coefficients[2, 1]) ## [1] 0.6575341 cor(data$y2, data$y1) ## [1] 0.6575341 Note the distinction between exogenous and endogenous linkages, which are represented with \\(\\gamma\\), and relationships between two endogenous variables, which are represented with \\(\\beta\\). 2.2.3 Rule 3: The strength of a compound path (one that includes multiple links) is the product of the individual coefficients. One of the strengths of SEM is being able to quantify indirect or cascading linkages. This is accomplished by simply multiplying the path coefficients. So the effect of \\(x1\\) on \\(y2\\) is the product of the coefficient of the path \\(x1 -&gt; y1\\) and \\(y1 -&gt; y2\\): pathx1_y1 * pathy1_y2 ## [1] 0.4579473 By our earlier logic, this value should equal the correlation between \\(x1\\) and \\(y2\\): cor(data$y2, data$x1) ## [1] 0.4484743 Ahh, but wait! The correlations are not the same. This result implies that the relationship between \\(x1\\) and \\(y2\\) cannot be fully explained by the indirect path through \\(y1\\). Rather, we require additional information to solve this problem, and it comes in the form of the missing link between \\(x1 -&gt; y2\\), which we can add to the model: Introducing this path raises a new issue: the relationship between \\(y1\\) and \\(y2\\) now arises from two sources. The first is their direct link, the second is the indirect effect of \\(x1\\) on \\(y2\\) through \\(y1\\). We require a new approach to be able to compute the independent effects of each variable on the others, which comes in the form of the partial regression coefficient. 2.2.4 Rule 4. When variables are connected by more than one pathway, each pathway is the partial regression coefficient. A partial regression coefficient accounts for the joint influence of more than one variable on the response. In other words, the coefficient for one predictor controls for the influence of other predictors in the model. In this new model, \\(y2\\) is affected by two variables: \\(x1\\) and \\(y1\\). Procedurally, this involves removing the shared variance between \\(x1\\) and \\(y1\\) so that their associations with \\(y2\\) can be independently derived. We can calculate the partial effect of \\(x1\\) through the following equation: \\[b_{y2x1} = \\frac{r_{x1y2} - (r_{x1y1} \\times r_{y1y2})}{1 - r_{x1y1}^2}\\] which takes the bivariate correlation between \\(x1\\) and \\(y2\\), removes the joint influence of \\(y1\\) and both \\(x1\\) on \\(y2\\), then scales this effect by the shared variance between \\(x1\\) and \\(y1\\). The result is the partial effect of \\(x1\\) on \\(y2\\). (partialx1 &lt;- (cor(data$x1, data$y2) - (cor(data$x1, data$y1) * cor(data$y1, data$y2))) / (1 - cor(data$x1, data$y1)^2)) ## [1] -0.0183964 It is important to note that partial coefficients implement a statistical (rather than experimental) control. In other words, the partial effect of \\(x1\\) accounts for the statistical contributions of \\(y1\\). Thus, partial effects are useful in situations where experimental controls are difficult or impossible to apply. Notably, we can back out the unstandardized coefficient by multiplying by the ratio of standard deviations of the response over the predictor: ((cor(data$x1, data$y2) - (cor(data$x1, data$y1) * cor(data$y1, data$y2))) / (1 - cor(data$x1, data$y1)^2))* (sd(data$y2)/sd(data$x1)) ## [1] -0.01754746 summary(lm(y2 ~ x1 + y1, data))$coefficients[2, 1] ## [1] -0.01754746 Similarly, the partial effect of \\(y1\\) on \\(y2\\) is given by: \\[b_{y2y1} = \\frac{r_{y2y1} - (r_{y2x1} \\times r_{y1x1})}{1 - r_{x1y1}^2}\\] (partialy1 &lt;- (cor(data$y2, data$y1) - (cor(data$y2, data$x1) * cor(data$y1, data$x1))) / (1 - cor(data$x1, data$y1)^2)) ## [1] 0.6703465 As before, we can arrive at the same answer by looking at the (standardized) coefficients obtained through multiple regression: summary(lm(y2 ~ x1 + y1, as.data.frame(apply(data, 2, scale))))$coefficients[2:3, 1] ## x1 y1 ## -0.0183964 0.6703465 partialx1; partialy1 ## [1] -0.0183964 ## [1] 0.6703465 The regression procedure therefore produces partial coefficients in the case of multiple predictors. Another way of thinking about this is that the effect of \\(x1\\) on \\(y2\\) is computed AFTER having removed the effect of \\(y1\\) on \\(y2\\). Procedurally, this can be done by first removing the variance in \\(x1\\) explained by \\(y1\\), then regressing the residual values (i.e., the variance in \\(y2\\) that is unexplained by \\(y1\\)) against \\(y2\\). residsx1 &lt;- residuals(lm(x1 ~ y1, as.data.frame(apply(data, 2, scale)))) summary(lm(scale(data$y2) ~ residsx1))$coefficients[2, 1] ## [1] -0.0183964 partialx1 ## [1] -0.0183964 Indeed, this iterative procedure gives us the same value as the former equation based on correlations and the output from the multiple regression. However, this raises the interesting notion of residual error. The second equation still has variance in \\(y2\\) that is unassociated with variation in either \\(x1\\) or \\(y1\\). In other words, the model does not perfectly predict \\(y2\\). The idea of residual (unexplained) variance leads us to the fifth rule of path coefficients. 2.2.5 Rule 5: Errors on endogenous variables relate the unexplained correlations or variances arising from unmeasured variables. Most researchers are familiar with the \\(R^2\\) statistic, or the ratio of explained variation to the total variation in \\(y2\\). It follows then that the unexplained or residual variance is \\(1 - R^2\\). For example, the error variance on \\(y2\\) is: 1 - summary(lm(y2 ~ y1 + x1, as.data.frame(apply(data, 2, scale))))$r.squared ## [1] 0.5674746 This value captures the other (unknown) sources that cause the correlation between \\(y2\\) and the other variables to deviate from 1. In other words, if \\(x1\\) is the only variable controlling \\(y2\\), then their correlation would be 1 and their prediction error would be 0 because we would have accounted for everything that affects \\(y2\\). This idea of residual or unexplained variance is easily illustrated with the relationship between \\(x1\\) and \\(y1\\) (because there are no partial correlations to deal with), where the square-root of variance explained is simply the correlation coefficient, and 1 - the correlation is the unexplained correlation arising from other sources: 1 - sqrt(summary(lm(y1 ~ x1, as.data.frame(apply(data, 2, scale))))$r.squared) ## [1] 0.3035383 1 - cor(data$y1, data$x1) ## [1] 0.3035383 In a path diagram, error variances are often represented as \\(\\zeta\\) with an arrow leading into the endogenous variable. The path coefficient representing the effect of \\(\\zeta\\) is often expressed as the error correlation: \\(\\sqrt(1 - R^2)\\), in keeping with the presentation of the other (standardized) coefficients. 2.2.6 Rule 6: Unanalyzed (residual) correlations among two endogenous variables are their partial correlations. Imagine we remove the path from \\(y1 -&gt; y2\\): Both variables are endogenous and their relationship can still be quantified, just not in a directed way. If they were exogenous variables, the relationship would be their bivariate correlation (Rule #1), but in this case, we have to remove the effects of \\(x1\\) on both variables. \\[r_{y1y2\\bullet x1} = \\frac{r_{y1y2} - (r_{x1y1} \\times r_{x1y2})}{\\sqrt((1 - r_{x1y1}^2)(1 - r_{x1y2}^2))}\\] This equation removes the effect \\(x1\\) and scales by the shared variance between \\(x1\\) and both endogenous variables. (errory1y2 &lt;- (cor(data$y1, data$y2) - (cor(data$x1, data$y1) * cor(data$x1, data$y2))) / sqrt((1 - cor(data$x1, data$y1)^2) * (1 - cor(data$x1, data$y2)^2))) ## [1] 0.5381952 As with the partial correlation, this value is the same as the correlation between the residuals of the two models: (cor( resid(lm(y1 ~ x1, as.data.frame(apply(data, 2, scale)))), resid(lm(y2 ~ x1, as.data.frame(apply(data, 2, scale)))) )) ## [1] 0.5381952 errory1y2 ## [1] 0.5381952 Hence these are known as correlated errors and are represented by double-headed arrows between the errors of two endogenous variables. (Often the errors or \\(\\zeta\\)s are omittedin the diagram, and the graph simply depicts a double-headed arrow between the variables themselves, but the correlation is truly among their errors.) If the presence of \\(x1\\) explains all of the variation in \\(y1\\) and \\(y2\\), then their partial correlation will be 0. In this case, the two endogenous variables are said to be conditionally independent, or that they are unrelated conditional on the joint influence of \\(x1\\). If the two are conditionally independent, then the correlation between \\(y1\\) and \\(y2\\) is the product of the correlations between \\(y1\\) and \\(x1\\), and \\(y2\\) and \\(x1\\). \\[r_{y1y2} = r_{y1x1} \\times r_{y2x1}\\] (If we replace this term in the previous equation, the numerator becomes 0 and so does the partial correlation.) Lets calculate this value: cor(data$y1, data$x1) * cor(data$y2, data$x1) ## [1] 0.3123451 Ah, the two are very different, implying that \\(y1\\) and \\(y2\\) are not conditionally independent given the joint influence of \\(x1\\). In other words, there are other, unmeasured sources of variance that are influencing the relationship between these two variables. This interpretation is a good way to think about the notion of correlated errors. The concept of conditional independence is critical in the implementation of local estimation, principally in the tests of directed separation that form the basis of one goodness-of-fit statistic, which we will revisit in Chapter 3. Now that we have derived all the quantities related to direct, indirect, and error variances or correlations, we have all the information necessary to calculate total effects. 2.2.7 Rule 7: The total effect one variable has another is the sum of its direct and indirect effects. If we return to our previous path model, which reinstates the path between \\(y1 -&gt; y2\\), the total effect of \\(x1\\) on \\(y2\\) includes the direct effect as well as the indirect effect mediated by \\(y1\\). (totalx1 &lt;- partialx1 + cor(data$y1, data$x1) * partialy1) ## [1] 0.4484743 This value can be used to demonstrate the final rule. 2.2.8 Rule 8: The total effect (including undirected paths) is equivalent to the total correlation. We can test this rule easily: totalx1 == cor(data$y2, data$x1) ## [1] TRUE Indeed, the total effect equals the correlation! If we consider the path model without the directed link between \\(y1\\) and \\(y2\\), the correlation between \\(y1\\) and \\(y2\\) considers the total effect and undirected effects (i.e., correlated errors between the two endogenous variables): (totaly1y2 &lt;- cor(data$y1, data$x1) * cor(data$y2, data$x1) + errory1y2 * sqrt(1 - summary(lm(y1 ~ x1, data))$r.squared) * sqrt(1 - summary(lm(y2 ~ x1, data))$r.squared)) ## [1] 0.6575341 totaly1y2 == cor(data$y1, data$y2) ## [1] TRUE This example closes our discussion of path coefficients and their 8 rules. The major points to remember are: standardized coefficients reflect (partial) correlations; the indirect effect of one variable on another is obtained by multiplying the individual path coefficients (standardized or unstandardized); the total effect is the sum of direct and indirect paths; the bivariate correlation is the sum of the total effect plus any undirected paths. An understanding of covariances and correlations is essential to understanding the solutions provided by a global estimation approach to SEM. 2.3 Variance-based Structural Equation Modeling The classical approach to SEM is based on the the idea of variance and covariances. With &gt;2 variables, we can construct a variance-covariance matrix, where the diagonals are the variances of each variable and the off-diagonals are the covariances between each pair. Consider our last example: cov(data) ## y1 x1 y2 ## y1 0.07602083 0.07970883 0.07178090 ## x1 0.07970883 0.17230020 0.07370632 ## y2 0.07178090 0.07370632 0.15676481 This code returns the variance-covariance matrix for the three variables \\(x1\\), \\(y1\\), and \\(y2\\). We would call this the observed global variance-covariance matrix. The entire machinery behind covariance-based SEM is to reproduce that global variance-covariance matrix. In fact, all of covariance-based SEM can be boiled down into this simple equation: \\[\\Sigma = \\Sigma(\\Phi)\\] where \\(\\Sigma\\) is the observed variance-covariance matrix, and \\(\\Sigma(\\Phi)\\) is the model-estimated covariance matrix expressed in terms of \\(\\Phi\\), the matrix of model-estimated parameters (i.e., coefficients). In other words, this equation shows that the observed covariances can be understood in terms of statistical parameters that can be used to predict these same covariances. We have already shown that linear regression can be expressed using variances and covariance, and so it follows that covariance-based SEM is simply a multivariate approach to regression. The question is: how do we arrive at \\(\\Sigma(\\Phi)\\), or more relevantly, how do we estimate the matrix of model parameters \\(\\Phi\\) that lead to the estimated variance-covariance matrix? The most common tool is maximum-likelihood estimation, which iteratively searches parameter space and continually refines estimates of parameter values such that the differences between the observed and expected variance-covariance matrices are minimized. The maximum-likelihood fitting function can be expressed as: \\[F_{ML} = log|\\hat{\\Sigma}| + tr(S\\hat{\\Sigma}^{-1}) - log|S| - (p + q)\\] where \\(\\Sigma\\) is the modeled covariance matrix, \\(S\\) is the observed covariance matrix, \\(p\\) is the number of endogenous variables, and \\(q\\) is the number of exogenous variables. \\(tr\\) is the trace of the matrix (sum of the diagonal) and the \\(^{-1}\\) denotes the inverse of the matrix. Maximum-likelihood estimators have a few desireable properties, principally that they are invariant to the scales of the variables and provide unbiased estimates based on a few assumptions: variables must exhibit multivariate normality. Oftentimes this is the not case: dummy variables, interactions and other product terms have non-normal distributions. However, \\(F_{ML}\\) is fairly robust to violations of multinormality, especially as the sample size grows large. the observed matrix \\(S\\) must be positive-definite. This means there are no negative variances, an implied correlation &gt; 1.0, or redundant variables (one row is a linear function of another). finally, \\(F_{ML}\\) assumes sufficiently large sample size. The notion of sample size is a good one: as models become increasingly complex, they require more data to fit. The issue of model identifiability and sample size is dealt with in the next section. 2.4 Model Identifiability Like any statistical technique, having sufficient power to test your hypotheses is key to arriving at robust unbiased inferences about your data. This requirement is particularly relevant to SEM, which often evaluates multiple hypotheses simultaneously, and therefore requires more data than other approaches. In this section, we will briefly review the idea of model identifiability and sample size requirements. A model is identified if we can uniquely estimate each of its parameters. This not only includes in the matrix of parameter estimates but also their errors. In other words, we need at least as many known pieces of information as unknowns at minimum to be able to fit a model. Consider the following equation: \\[a + b = 8\\] We have 1 piece of known information, \\(8\\), and two unknowns, \\(a\\) and \\(b\\). There are a number of solutions for \\(a\\) and \\(b\\) (e.g., 1 and 7, 2 and 6, etc), and thus the equation is not solvable. In this case, the model would be underidentified because we lack sufficient information to arrive at a unique solution. Now consider another equation, in addition to the first: \\[a = 3b\\] With this equation, we now have enough information to uniquely solve for \\(a\\) and \\(b\\): \\[(3b) + b = 8\\] \\[ 4b = 8\\] \\[b = 8 / 4 = 2\\] \\[a + 2 = 8\\] \\[a = 8 - 2 = 6\\] Thus we have arrived at a single solution for \\(a\\) and \\(b\\). We call this system of equations just identified since we have just enough information to solve for the unknowns. Finally, consider a third equation: \\[2a - 4 = 4b\\] We now have more pieces of known information than unknowns, since we have already arrived at a solution for both \\(a\\) and \\(b\\) based on the previous two equations. In this case, we call the system of the equations overidentified because we have more information than is necessary to arrive at unique solutions for our unknown variables. This is the desireable state, because that extra information can be used to provide additional insight, which we will elaborate on shortly. You may have also heard of models referred to as saturated. Such a model would be just identified; an unsaturated model would be overidentified and and an oversaturated model would be underidentified. There is a handy rule that can be used to quickly gauge whether a model is under-, just, or overidentified: the t-rule. The t-rule takes the following form: \\[t \\leq \\frac{n(n+1)}{2}\\] where \\(t\\) is the number of unknowns (parameters to be estimated) and \\(n\\) is the number of knowns (observed variables). The left hand side is how many pieces of information we want to know. The right hand side reflects the information we have to work with and is equal to the number of unique cells in the observed variance-covariance matrix (diagonal = variance, and lower triangle = covariances). Consider the simple mediation model from earlier: In this model, we have several pieces of known information: \\(x1\\), \\(y1\\), and \\(y2\\). So \\(n = 3\\) for this model. We need to estimate the parameters related each set of relationships (\\(\\gamma_{x1y1}\\) and \\(\\beta_{y1y2}\\)). These amount to the two covariances, but recall from the first section of this chapter that we need three variances to derive those estimates (\\(var_{x1}\\), \\(var_{y1}\\), \\(var_{y2}\\)). So the total number of unknowns to be estimated is \\(t = 5\\) (the three variances and the two regression parameters, or covariances). We can plug in these values to see if we meet the t-rule: \\[5 \\leq \\frac{3(3+1)}{2} = 6\\] In this case \\(5 \\leq 6\\) holds true and we have enough information to arrive at a unique solution. Note that the right hand side of the equation is the number of entries in the variance-covariance matrix: 3 variances (diagonal) plus 3 covariances (off-diagonal). Lets consider our second model, which adds another path: Now we must additionally estimate the path from \\(x1\\) to \\(y2\\) so our value of \\(t = 5 + 1\\). However, \\(6 \\leq 6\\) and so the t-rule is still satisfied. In this case, however, the model is just identified. Tallying the number of parameters to be estimated can sometimes be tricky because path diagrams are not always drawn with error variances on the endogenous variables. Additionally, multiple exogenous variables also have a covariance that must be estimated: this is depicted as a non-directional or double-headed error between every pair of exogenous variables. However, these double-headed arrows are rarely drawn, even though they exist. Thus it can be tricky to identify \\(n\\) in the above equation. In such cases, it is valid to simply count the number of unique cells in the variance-covariance matrix (the diagonal and the lower off-diagonal). If we were to consider a more complex model, such as one with a feedback from \\(y2\\) to \\(y1\\) (in addition to the path from \\(y1 -&gt; y2\\)) then we would not have enough information to solve the model, which would be underidentified. Models with bidirectional feedbacks (with separate arrows going in each direction, as opposed to a single double-headed arrow) are referred to as non-recursive. These feedbacks can also occur among variables, for instance: \\(x1 -&gt; y1 -&gt; y2 -&gt; x1\\) would also be a non-recursive model. Recursive models, then, lack such feedbacks and all causal effects are unidirectional. Identifiability of non-recursive is tricky. Such models must satisfy the order condition. This condition tests whether variables involved in the feedback have unique information. In our above example of \\(y2\\) also affecting \\(y1\\), \\(y1\\) has unique information in the form of \\(x1\\) but \\(y2\\) has no unique information, so it fails the order condition. The order condition can be evaluated using the following equation: \\[G \\leq H\\] where \\(G\\) = the number of incoming paths, and \\(H\\) = the number of exogenous variables + the number of indirectly-connected endogenous variabls. In the previous example, \\(G = 2\\) while \\(H = 1\\), so the model fails the order condition, as noted. Model identification is only the first step in determining whether a model can provide unique solutions: sample size can also restrict model fitting by not providing enough replication for the \\(F_{ML}\\) function to arrive at a stable set of estimates for the path coefficients. The basic rule-of-thumb is that the level of replication should be at least 5 times the number of estimated coefficients (not error variances or other correlations). So in our previous path model, we are estimating two relationships, so we require at least \\(n = 10\\) to fit that model. However, this value is a lower limit: ideally, replication is 5-20x the number of estimated parameters. The larger the sample size, the more precise (unbiased) the estimates will be. This is true for all linear regressions, not just SEM, and so we adopt these guidelines here. Identifiability and replication are key in not only providing an actual solution, but also in providing extra information with which to evaluate model fit, the topic of the next section. 2.5 Goodness-of-fit Measures As we have established, the purpose of covariance-based SEM is to reproduce the global observed variance-covariance matrix. However, given that our hypothesized relationships may not actually match the data, we must be prepared to evaluate how well the model-estimated variance-covariance matrix matches the observed variance-covariance matrix. Recall that in the previous section on Path Coefficients, we evaluated the error variance or correlation as reflecting outside sources of variation uncaptured by our measured variables. High error variances would lead to less accurate estimates of the relationships among variables, and thus a high level of disagreement among the observed and model-implied variance-covariance matrix. Also recall our formula for the maximum-likelihood fitting function: \\[F_{ML} = log|\\hat{\\Sigma}| + tr(S\\hat{\\Sigma}^{-1}) - log|S| - (p + q)\\] where \\(\\Sigma\\) is the modeled covariance matrix, \\(S\\) is the observed covariance matrix, \\(p\\) is the number of endogenous variables, and \\(q\\) is the number of exogenous variables. \\(tr\\) is the trace of the matrix (sum of the diagonal) and the \\(^{-1}\\) is the inverse of the matrix. In the event that \\(\\Sigma = S\\), then the first two terms would equal 0, and similarly for the second two terms. Thus a model where \\(F_{ML} = 0\\) implies perfect fit because the observed covariance matrix has been exactly reproduced. Oppositely, a large value of \\(F_{ML}\\) would imply increasing discrepancy between the observed and model-implied variance-covariance matrices. This could be interpreted as a poor fit for the model. In fact, \\(F_{ML}\\) is \\(\\chi^2\\)-distributed such that: \\[\\chi^2 = (n - 1)F_{ML}\\] which allows us to actually quantify model fit. (In this case \\(n\\) refers to the sample size.) We can then formally compare the \\(\\chi^2\\) statistic to the \\(\\chi^2\\)-distribution with some degrees of freedom to achieve a confidence level in the fit. The degrees of freedom are determined by model identification: just identified models will have 0 degrees of freedom (and thus no goodness-of-fit test is possible), and identified models will have \\(n(n + 1)/2 - t\\) degrees of freedom (where \\(n\\) are the number of knowns and \\(t\\) the number of unknowns, from the t-rule). Failing to reject the null hypothesis that the \\(\\chi^2\\) statistic is different from 0 (the ideal fit) implies a generally good representation of the data (P &gt; 0.05). Alternatively, rejecting the null implies that the \\(\\chi^2\\) statistic is large, as is the discrepancy between the observed and modeled variance-covariance matrices, thus implying a poor fit to the data (P &lt; 0.05). Interpreting the outcome of the significance test is often tricky: unlike testing whether regression parameters are significantly different from zero (something we are all familiar with), in this case, a significant P-value indicates poor fit, so be careful. The \\(\\chi^2\\) index also provides a way to gauge the relative fit of two models, one of which is nested within the other. The \\(\\chi^2\\) difference test is simply the difference in \\(\\chi^2\\) values between the two models, with the degrees of freedom being the difference in the degrees of freedom between the two models. The resulting statistic can then be compared to a \\(\\chi^2\\) table to yield a significance value. Again, this test is for nested models. For non-nested models, other statistics allow for model comparisons, including AIC and BIC. An AIC or BIC score \\(\\geq2\\) is generally considered to indicate significant differences among models, with smaller values indicating equivalency between the two models. \\(\\chi^2\\) tests tend to be affected by sample size, with larger samples more likely to generate poor fit due to small absolute deviations (note the scaling of \\(F_{ML}\\) by \\(n-1\\) in the above equation). As a result, there are several other fit indices for covariance-based SEM that attempt to correct for this problem: Root-mean squared error of approximation (RMSEA): this statistic penalizes models based on sample size. An acceptable value is generally &lt;0.10 and a good value is anything &lt;0.08. Comparative fit index (CFI): this statistic considers the deviation from a null model. In most cases, the null estimates all variances but sets the covariances to 0. A value &gt;0.9 is considered good. Standardized root-mean squared residual (SRMR): the standardized difference between the observed and predicted correlations. A value &lt;0.08 is considered good. There are a vast and probably unnecessary number of other fit statistics that have been developed which you may run across. This website has a fairly comprehensive overview, in the event you encounter some of the more uncommon ones in the literature. What happens if the model doesnt fit? Depending on the goals of your analysis (e.g., exploratory) you may wish to see which parts of your model have failed to be reproduced by the model-implied variance-covariance matrix. This can be achieved in two ways: examination correlation of model residuals: parameters with large residual correlations (difference between observed and expected) could suggest missing information or linkages. modification indices, or the expected decrease in the \\(\\chi^2\\) if a missing path were to be included in the model. A high value of a modification index would suggest the missing path should be included. (Tests of directed separation, which we cover in the chapter on Local Estimation, provide similar insight and are returned automatically by piecewiseSEM.) Users should take caution when exploring these techniques as to avoid dredging the model. SEM is a technique that relies heavily on informed model specification. Adding paths in that are suggested by the data but not anticipated by the user only in an effort to achieve adequate fit might be appropriate in other applications, but ignore the basic philosophy behind SEM that relationships are based on a priori knowledge and intuition. 2.6 Model Fitting Using lavaan We now have all the pieces necessary to fit an SEM using a covariance-based approach. The package to do so is called lavaan: library(lavaan) To demonstrate the functionality of this package, lets use the data from Grace &amp; Keeley (2006), which is included in the piecewiseSEM package: library(piecewiseSEM) data(keeley) In their study, Grace &amp; Keeley wanted to understand patterns in plant diversity following disturbance, in this case wildfires in California. 2.6.1 lavaan vs lm For purposes of exploration, lets first consider the relationship between fire severity and stand age (with older stands having more combustible materials). This is both a linear regression but also a simple SEM, and thus both can be fit using packages in R. The package to fit the SEM using covariance-based methods is called lavaan (for LAtent VAriable ANalysis, which we will delve into in a later chapter). In lavaan, the syntax is the same as in other modeling functions in R with one key distinction: formulae are passed as character strings. To fit a model in lavaan, its first necessary to break down the component models by the endogenous (response) variables and code them as characters. For example: keeley_formula1 &lt;- &#39;firesev ~ age&#39; class(keeley_formula1) ## [1] &quot;character&quot; The function used to fit the model is called (unsurprisingly) sem and accepts the formula string and the dataset: keeley_sem1 &lt;- sem(keeley_formula1, data = keeley) As with most other models, the function to retrieve the output is summary: summary(keeley_sem1) ## lavaan 0.6-9 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## firesev ~ ## age 0.060 0.012 4.832 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .firesev 2.144 0.320 6.708 0.000 The output is organized into a few sections. First is the likelihood optimization method, number of parameters, the total sample size for the model, the estimator (\\(F_{ML}\\) is the default) and the fit statistic. The model has \\(\\chi^2 = 0\\) with 0 degrees of freedom: this is because we have as many knowns as unknowns, and thus the model is just identified or saturated. To show this, we can apply the t-rule: we must estimate the two variances of the variables plus their path coefficient (\\(t = 3\\)) and know the values of the two variables (\\(n = 2\\)). Recall the equation for the t-rule \\(t \\leq n(n + 1)/2\\), so \\(3 = 2(2+1)/2 = 6/2 = 3\\), and therefore the model is saturated. Next up are the actual parameter estimates: the relationship between fire severity and stand age is \\(\\beta = 0.06\\) with \\(P &lt; 0.001\\). The model also reports the estimated error variance on the endogenous variable. We can dissect this output a little more. First, lets fit the corresponding linear model using lm: keeley_mod &lt;- lm(firesev ~ age, data = keeley) summary(keeley_mod)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.03920623 0.35543253 8.550726 3.448774e-13 ## age 0.05967903 0.01249008 4.778113 7.027847e-06 Youll notice that we get the same effect of age on fire severity \\(\\beta = 0.0596\\), but we also get an intercept, which is missing from the previous input. We can force lavaan to return the intercept using the argument meanstructure = T to the sem function: summary(sem(keeley_formula1, keeley, meanstructure = T)) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## firesev ~ ## age 0.060 0.012 4.832 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .firesev 3.039 0.351 8.647 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .firesev 2.144 0.320 6.708 0.000 which now returns the estimate of the intercept for fire severity. This value is the same as returned by lm. Returning to our exploration of how path coefficients are calculated, the slope of a simple linear regression is \\(b_{xy} = COV_{xy}/VAR_{x}\\), which we can recover from the raw data: cov(keeley[, c(&quot;firesev&quot;, &quot;age&quot;)])[2, 1]/var(keeley$age) ## [1] 0.05967903 Note that this value is the same returned from both sem and lm. Recall also for simple linear regression that the standardized coefficient is equal to the correlation: cor(keeley$firesev, keeley$age) ## [1] 0.4538654 We can obtain the standardized coefficient from lm using the coefs function from piecewiseSEM: coefs(keeley_mod) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 firesev age 0.0597 0.0125 88 4.7781 0 0.4539 *** and indeed, it equals the bivariate correlation! To return the standardized coefficients using lavaan requires a separate function or another argument. The function is standardizedsolution and returns a table of the standardized coefficients: standardizedsolution(keeley_sem1) ## lhs op rhs est.std se z pvalue ci.lower ci.upper ## 1 firesev ~ age 0.454 0.079 5.726 0 0.299 0.609 ## 2 firesev ~~ firesev 0.794 0.072 11.035 0 0.653 0.935 ## 3 age ~~ age 1.000 0.000 NA NA 1.000 1.000 This output does not return the raw coefficients, however, or any other information about the model that is useful in interpretation. The obtain a single output, you can pass the argument standardize = T to summary: summary(keeley_sem1, standardize = T) ## lavaan 0.6-9 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.060 0.012 4.832 0.000 0.060 0.454 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 Now, a few columns are added at the end to report the standardized coefficients. standardizedsolution also returned the error variance on fire severity, which is \\(1 - R^2\\). However, lavaan also doesnt return the \\(R^2\\) value by default, but can be retrieved using one more argument to summary, rsq = T: summary(keeley_sem1, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.060 0.012 4.832 0.000 0.060 0.454 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 ## ## R-Square: ## Estimate ## firesev 0.206 Youll note that, per Rule 5 of path coefficients, the error variance is \\(1 - R^2\\). 2.6.2 SEM using lavaan Now that we have covered the basics of lavaan, lets fit a slightly more complicated SEM. This model is a simplified subset of the full model presented by Grace &amp; Keeley: Here, we test the hypotheses that total cover of plants is a function of fire severity, which in turn is informed by how old the plants are in a particular plot (which we have already investigated using linear regression). This test is known as full mediation. In other words, the effect of age is fully mediated by fire severity (we will test another scenario shortly). Again, we must provide the formulae as a character string. This model can be broken down into two equations (placed on separate lines) representing the two endogenous variables: keeley_formula2 &lt;- &#39; firesev ~ age cover ~ firesev &#39; NOTE: Multiple equations will go on separate lines so that lavaan can properly parse the model. Now lets fit the SEM and examine the output: keeley_sem2 &lt;- sem(keeley_formula2, data = keeley) summary(keeley_sem2, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 3.297 ## Degrees of freedom 1 ## P-value (Chi-square) 0.069 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.060 0.012 4.832 0.000 0.060 0.454 ## cover ~ ## firesev -0.084 0.018 -4.611 0.000 -0.084 -0.437 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 ## .cover 0.081 0.012 6.708 0.000 0.081 0.809 ## ## R-Square: ## Estimate ## firesev 0.206 ## cover 0.191 The key difference from our previous application of lavaan is we now have extra information with which to compute the \\(\\chi^2\\) goodness-of-fit statistic. A quick application of the t-rule: unknowns = 3 variances + 2 path coefficients = 5, knowns = 3 variables, so \\(5 &lt; 3(3+1)/2 = 6\\) leaving us 1 extra degree of freedom. Moreover, we fail to reject the null that the observed and model-implied variance-covariance matrices are significantly different (\\(P = 0.069\\)). Thus, we have achieved adequate fit with this model. A note here that the magnitude of the P-value does not reflect increasing fit, only increasing support for the notion that the observed and model-estimated covariances are not different. Incidentally, we can obtain other fit statistics using the fitMeasures function: fitMeasures(keeley_sem2) ## npar fmin chisq df ## 4.000 0.018 3.297 1.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.069 43.143 3.000 0.000 ## cfi tli nnfi rfi ## 0.943 0.828 0.828 0.771 ## nfi pnfi ifi rni ## 0.924 0.308 0.945 0.943 ## logl unrestricted.logl aic bic ## -176.348 -174.699 360.696 370.695 ## ntotal bic2 rmsea rmsea.ci.lower ## 90.000 358.071 0.160 0.000 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.365 0.101 0.245 0.245 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.062 0.062 0.062 0.088 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.088 0.062 0.062 105.849 ## cn_01 gfi agfi pgfi ## 182.093 0.966 0.798 0.161 ## mfi ecvi ## 0.987 0.126 Woah! Were certainly not lacking in statistics. Returning to the summary output, we see the same coefficient for \\(firesev ~ age\\) and a new estimate for \\(cover ~ firesev\\). In this case, more severe fires reduce cover (not unexpectedly). Now that we have multiple linkages, we can also compute the indirect effect of age on cover. Recall from Rule 3 of path coefficients that the indirect effects along a compound path are the product of the individual path coefficients: \\(0.454 * -0.437 = -0.198\\). We can obtain this value by modifying the model formula to include these calculations directly. This involves giving a name to the coefficients in the model strings, then adding a new line indicating their product using the operator :=: keeley_formula2.1 &lt;- &#39; firesev ~ B1 * age cover ~ B2 * firesev indirect := B1 * B2 &#39; keeley_sem2.1 &lt;- sem(keeley_formula2.1, keeley) summary(keeley_sem2.1, standardize = T) ## lavaan 0.6-9 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 3.297 ## Degrees of freedom 1 ## P-value (Chi-square) 0.069 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age (B1) 0.060 0.012 4.832 0.000 0.060 0.454 ## cover ~ ## firesev (B2) -0.084 0.018 -4.611 0.000 -0.084 -0.437 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 ## .cover 0.081 0.012 6.708 0.000 0.081 0.809 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## indirect -0.005 0.002 -3.336 0.001 -0.005 -0.198 Indeed, the indirect path coefficient is the same as computed above. Naming coefficients can come in handy when specifying, for example, fixed values for specific purposes (e.g., multigroup modeling). 2.6.3 Testing Alternative Structures using lavaan There is another possible configuration of these variables which includes a directed path between age and cover: This type of model tests partial mediation, or the idea that the effect of age is partially mediated by fire severity, but there is still a direct effect between age and cover. Lets fit the partial mediation model: keeley_formula3 &lt;- &#39; firesev ~ age cover ~ firesev + age &#39; keeley_sem3 &lt;- sem(keeley_formula3, data = keeley) summary(keeley_sem3, standardize = T) ## lavaan 0.6-9 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.060 0.012 4.832 0.000 0.060 0.454 ## cover ~ ## firesev -0.067 0.020 -3.353 0.001 -0.067 -0.350 ## age -0.005 0.003 -1.833 0.067 -0.005 -0.191 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 ## .cover 0.078 0.012 6.708 0.000 0.078 0.780 Ah, we have a problem: the model is saturated, so there are no degrees of freedom leftover with which to test model fit. We can, however, use the \\(\\chi^2\\) test to determine whether this model is more or less supported than the partial mediation model, which is called using the function anova: anova(keeley_sem2, keeley_sem3) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## keeley_sem3 0 359.4 371.9 0.0000 ## keeley_sem2 1 360.7 370.7 3.2974 3.2974 1 0.06939 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Note that the \\(\\chi^2\\) statistic and associated degrees of freedom are still 0 for the model testing partial mediation.) We can see from this output that we fail to reject the null that the models are significantly different (\\(P = 0.069\\)). Because these are nested models, it is also fair to compare them using AIC/BIC. In both cases, the models are deemed equivalent and the more parsimonious model (full mediation) would be preferred. Moreover, examining the output of the model of partial mediation reveals a non-significant effect of age on cover (\\(P = 0.067\\)). Together, these pieces of information would suggest that plant cover is not directly affected by stand age, but rather the effect of age is entirely mediated through the severity of burns. 2.7 References Grace, J. B., &amp; Keeley, J. E. (2006). A structural equation model analysis of postfire plant diversity in California shrublands. Ecological Applications, 16(2), 503-514. "],["local-estimation.html", "3 Local Estimation 3.1 Global vs. local estimation 3.2 Tests of directed separation 3.3 A Log-Likelihood Approach to Assessing Model Fit 3.4 Model fitting using piecewiseSEM 3.5 Extensions to Generalized Mixed Effects Models 3.6 Extensions to Non-linear Models 3.7 A Special Case: Where Graph Theory Fails 3.8 References", " 3 Local Estimation 3.1 Global vs. local estimation In the previous chapter, we explored the use of structural equation modeling to estimate relationships among a network of variables based on attempts to reproduce a single variance-covariance matrix. We refer to this approach as global estimation because the variance-covariance matrix captures relationships among all variables in the model at once. This previous approach comes with a number of assumptions about the data, notably that they are multivariate normal and sufficiently replicated to generate unbiased parameter estimates. However, most dataparticularly ecological dataviolate these assumptions. Given the difficulty with which the data are collected and the complexity of the proposed relationships, issues with power are often encountered. While variance-covariance based methods have been extended to consider special cases such as non-normality, an alternative estimation procedure was proposed in 2000 by Shipley based on concepts from graph theory. In this method, relationships for each endogenous (response) variable are estimated separately, which is why we call it local estimation or piecewise SEM, due to the nature by which the model is pieced together. Recall that global estimation assumes linear relationships, and indeed we have seen in the previous chapter that fitting a SEM and comparing the output with that from a linear model can yield the same results. Local estimation takes the latter approach: fitting a linear model for each response and then stringing together the inferences, rather than trying to estimate all relationships at once. Thus, piecewise SEM is more like putting together a puzzle piece by piece, rather than painting the image on a single canvas. This approach imparts great flexibility because the assumptions pertaining to each response can be evaluated and addressed individually rather than treating every variable as arising from the same data-generating process. For example, generalized linear models can be fit for data that are non-Gaussian such as count (e.g., abundance), proportion (e.g., survival), or binary outcomes (e.g., presence-absence). Mixed-effects or hierarchical models can be fit for data that are nested or adhere to some predefined structure. Similarly, non-independence (such as spatial, temporal, or phylogenetic) can be incorporated into the model structure to provide more robust parameter estimates. Moreover, only enough data is needed to be able to fit and estimate each individual regression. In doing so, Shipleys method relaxes many of the assumptions associated with global estimation and better reflects the types and quantity of data collected by modern ecologists. A key point to be made is that the piecewise approach does not absolve the user of all assumptions associated with the statistical tests. The data must still meet the assumption of the individual models: for example, most linear regression assume constant variance and independence of errors. Such assumptions still hold, but they can be easily evaluated using the suite of tools already available for said models (e.g., histograms of residuals plots, Q-Q plots, etc.). However, recall that the goodness-of-fit measures for variance-covariance based structural equation models derive from comparison of the observed vs. estimated variance-covariance matrix. Because local estimation produces a separate variance-covariance matrix for each modeled response, there is no immediate extension from global methods. Instead, Shipley proposed a new test based on directed acyclic graphs (or DAGs). DAGs are the pictorial representation of the hypothesized causal relationships: in other words, the path diagram. Its important to point out quickly that DAGs assume recursive relationships, or the absence of feedback loops or bidirectional relationships. Thus, local estimation is unsuitable for such approaches and one must resort to a global approach (with some additional conditions for such model structures). There is a rich literature pertaining to DAGs, principally in their estimation and evaluation, and Shipley has drawn on this to propose a new index of model fit. More recently, Shipley and Douma have introduced a more flexible method based on log-likelihood that produces a \\(\\chi^2\\) statistic whose interpretation is much closer to that of global estimation, but further relaxes assumptions by allowing for model assessment and comparison for any models fit using maximum likelihood estimation. 3.2 Tests of directed separation In global estimation, comparison of the observed vs. estimated variance-covariance matrix through the \\(\\chi^2\\) statistic asks whether the model-implied relationships deviate substantially from the relationships present in the data. If not, then the model is assumed to fit well, and we can go on to use it for inference. Another way of thinking about model fit is to ask: are we missing any important paths? Recall that structural equation modeling requires careful specification of a hypothesized structure. In the case of underidentified models (those where there are fewer pieces of known information than parameters to be estimated), this means there are missing relationships that could be present but were not included. Paths might be excluded because there is no a priori reason or mechanism to suspect a causal relationship. Recall that modification indices can be used to test the change in the \\(\\chi^2\\) statistic (i.e., how well the model fits) with the inclusion of these missing paths. The tests of directed separation evaluate this hypothesis more directly by actually fitting the missing relationships to test whether the path coefficients are significantly different from zero, and there whether we are justified in excluding them. This question is actually implicit in the \\(\\chi^2\\) statistic: a substantial deviation from the observed correlations suggests that were missing information in our model that could bring our estimates more in line with our observations. Two variables are said to be d-separated if they are statistically independent conditional on their joint influences. Lets unpack this statement: First, the two variables are unrelated in the hypothesized causal model: in other words, there is not a directed path already connecting them. Second, we test for statistical dependence in our model all the time: the P-values associated with the path coefficients, for example, test whether the effect is significantly different than zero. Statistical independence then asks whether the two variables are significantly unrelated, or that that their relationship is in fact no different from zero. Finally, conditional on their joint influences means that the test for statistical independence must account for contributions from already specified influences. In other words, the test must consider the partial effect of one variable on the other if either or both are already connected to other variables in the model. Procedurally, this evaluation is quite easy: identify the sets of missing relationships, test whether the effect is not significantly different from zero (P &gt; 0.05) controlling for covariates already specified in the model, and combine those inferences to gauge the overall trustworthiness of the model. Lets consider a simple path diagram: In this case, we have specified two sets of directed relationships: \\(x1 -&gt; y1\\) and \\(y1 -&gt; y2\\). If we apply the t-rule from the chapter on global estimation, we have \\(3(3+1)/2\\) or 6 pieces of known information (the variances on the 3 variables + the 3 sets of covariances). We want to estimate the 2 parameters \\(\\gamma_{x1y1}\\) and \\(\\beta_{y1y2}\\) and the variances on the 3 variables (we can get their covariances from that). Thus we have 6 known values to estimate 5 unknown values, and the model is overidentified. We noted in the chapter on global estimation that the number of leftover known values can be used as degrees of freedom in the \\(\\chi^2\\) goodness-of-fit test. In this case, there is 1 degree of freedom, so likewise, we can go on to test model fit. This 1 degree of freedom actually corresponds to the missing relationship between \\(x1 -&gt; y2\\). This is the independence claim we wish to test: that there is in fact no relationship between \\(x1\\) and \\(y2\\). However, the effect of \\(x1\\) on \\(y2\\) must be independent (or the partial effect) of the known influence of \\(y1\\). Thus, we are testing the partial effect of \\(x1\\) on \\(y2\\) given \\(y1\\). You may see this claim written in the following notation: \\(x1 | y2 (y1)\\) where the bar separates the two variables in the claim, and any conditioning variables follow in parantheses. (Shipley actually puts the two variables in parentheses followed by the conditioning variables in brackets: \\((x1, y2) | {y1}\\), for the record.) In this simple example, there is one conditioning variable for the single independence claim. This one independence claim constitutes what is called the basis set, which is the minimum number of independence claims derived from a path diagram. The key word to take away here is minimum. We could have just as easily tested the claim \\(y2 | x1 (y1)\\), which is the same relationship but in the opposite direction. However, the statistical test or P-value associated with this relationship is the same regardless of the direction. In other words, the partial effect of \\(x1\\) on \\(y2\\) is the same as \\(y2\\) on \\(x1\\) (although there is a caveat to this claim for GLM, which we will address later). In such a case, we would include only the one claim rather than both claims that provide the same information. Therefore, our first rule of directed separation is: the sum number of independence claims in the basis set cannot be derived from some combination of the others within it. As an aside, if we add this claim back into the model, we would have no missing paths. Thus, no independence claims or tests of directed separation would be possible. As is the case with \\(\\chi^2\\), we would not have any leftover information with which to test model fit. This does not preclude fitting the model and drawing inference, only that its goodness-of-fit cannot be assessed. However, there are other qualitative ways of assessing model fit, such as looking that proportion of variance or deviance explained for each endogenous variable (i.e., \\(R^2\\)) and assessing the significance of the individual paths. If a high proportion of variance is explained in all endogenous variable and there are significant path coefficients, it follows that residual error is low, and its safe to assume that there are no other variables out there that can further clarify the model structure. Nevertheless, you should be open about why you chose to fit a just identified model, and why you are not reporting any goodness-of-fit statitsics. As path diagrams become more complex, the natural question is: how far back do you go in terms of conditioning? Take the following example: There are several missing paths: \\(x1 -&gt; y2\\), \\(x1 -&gt; y3\\) and \\(y1 -&gt; y3\\). Lets consider the independence claim \\(x1 -&gt; y3\\). Based on our last example, \\(y2\\) must be included as a conditioning variable due to its direct influence on \\(y3\\), but what about \\(y1\\)? It has an indirect influence on \\(y3\\) through \\(y2\\). However, by having included \\(y2\\) in the independence claim, we have already (theoretically) incorporated the indirect influence of \\(y1\\) in the form of variation in \\(y2\\). In other words, any effect of \\(y1\\) would change \\(y2\\) before \\(y3\\), and the variance in \\(y2\\) is already considered in the independence claim. So the full claim would be simply: \\(x1 | y3 (y2)\\). Our second rule of the d-sep test is: conditioning variables consist of only those variables immediate to the two variables whose independence is being evaluated. In other words, we assume that the effects of any other downstream variables are captured in the variance contributed by the immediate ancestors, and we can therefore ignore them. Upstream variables (those occurring later in the path diagram beyond both variables included in the claim) are never considered as conditioning variables, for the obvious reason that causes cannot precede effects. For the claim \\(y1 -&gt; y3\\) above, there are now two conditioning variables: \\(y2\\) (on \\(y3\\)) and also \\(x1\\) (on \\(y1\\)). So the final independence claim would be: \\(y1 | y3 (x1, y1)\\). Note that the effect of \\(y1\\) on \\(y2\\) is not included, because it is too ancestral. The full basis set for this diagram would then be: \\(x1 | y3 (y2)\\) \\(y1 | y3 (y2, x1)\\) \\(x1 | y2 (y1)\\) Deriving the basis set can be difficult but mercifully is automated in the piecewiseSEM package. This package makes some choices about the basis set that deviate from the recommendations of Shipley. For example, consider the following path diagram: The basis set includes the unspecified paths from \\(x1 | y2 (y1)\\) and \\(x2 | y2 (y1)\\). But what about \\(x1 | x2\\)? Shipley would include this claim in the basis set. However, several argument could be made against it along several fronts. First, unlike \\(y2\\) which very clearly is an effect (i.e., has a directed path flowing into it), there is no expectation of a cause-effect relationship between the two exogenous variables \\(x1\\) and \\(x2\\). In fact, such a relationship may yield nonsensical claims (e.g., between study area and month) or claims where directionality is confounded in one direction (e.g., latitude drives variation in species richness, but species richness does not change the latitude of the Earth). If the purpose of the test is to evaluate linkages that were originally deemed irrelevant, is it really that useful to test non-mechanistic or impossible links? If we did indeed recover a significant correlation between study area and month, is that mechanistically meaningful? Why should the area over which the study was conducted change due to the time of the study? And should we therefore reject a model due to a totally spurious relationship? These are tough questions with no clear answer. From a personal perspective, I believe the tests of directed separation should be diagnostic: should I have included this path? Did it provide useful information? Including non-informative claims because they can be evaluated simply inflates type II error (i.e., you are more likely to falsely accept the model) with no real benefit to the identifying underlying causal processes. Second, and more practically, there is no easy way for the user to specify the distributional and other assumptions associated with exogenous variables in the same way they can for endogenous variables. By virtue of modeling \\(y2\\) in a directed path (from \\(y1\\)), the user has made it clear in the coding of the model how that response should be treated. However, no where in the regression models is there information on how \\(x1\\) should be treated: is it binomial? Hierarchical? Polynomial? Asking the user to code this information would vastly inflate the amount of code necessary to run tests, and combined with the above, would yield little insight for a potentially very large hindrance. Nevertheless, independence claims could be added back into the basis set if the user decides they disagree with this perspective. Now that we are comfortable identifying missing paths and constructing the basis set, the next step is to test them for statistical independence. This can be done by taking the response as it is treated in the original model, and swapping the predictors with those in the independence claim. The way, the assumptions of the endogenous variable are preserved. So, for example, if \\(y3\\) in the previous path model is binomally-distributed as a function of \\(y2\\), then any independence claims involving \\(y2\\) would also treat is as binomial. Once the model is fit, statistical independence is assessed with a t-, F-, or other test. If the resulting P-value is &gt;0.05, then we fail to reject the null hypothesis that the two variables are conditionally independent. In this case, a high P-value is a good thing: it indicates that we were justified in excluding that relationship from our path diagram in the first place, because the data dont support a strong linkage between those variables within some tolerance for error. Shipleys most important contribution was to show that the P-values can be summed to construct a fit index analogous to the \\(\\chi^2\\) statistic from global estimation: Fishers C statistic, which is calculated as: \\[C = -2{\\sum_{i=1}^k ln(p_i)}\\] where \\(k\\) is the number of independence claims in the basis set, i is the ith claim, and p is the P-value from the corresponding significance test. Furthermore, Shipley showed that C is \\(\\chi^2\\) distributed with 2\\(k\\) degrees of freedom. Thus, a model-wide P-value can be obtained by comparing the value to a \\(\\chi^2\\) table with the appropriate degrees of freedom. As with the \\(\\chi^2\\) test in global estimation, a model-wide P &gt; 0.05 is desirable because it implies that the hypothesized structure is supported by the data. In other words, no potentially significant missing paths were excluded. Like the \\(\\chi^2\\) difference test, the C statistic can be used to compare nested models. Shipley later showed that the the C statistic can also be used to compute an AIC score for the SEM: \\[AIC = C + 2K\\] where \\(K\\) is the likelihood degrees of freedom (not \\(k\\), the number of claims in the basis set). A further variant for small sample sizes, \\(AIC_c\\), can be obtained by adding an additional penalty: \\[AIC_c = C + 2K\\frac{n}{(n - K - 1)}\\] Its important to point out that, like the \\(\\chi^2\\) statistic for global estimation, the C statistic can be affected by sample size, but not in as direct a way. As sample size increases, the probability of recovering a significant P-vaue increases, reducing the potential for a good-fitting model. Similarly, more complex models may lead to a kind of overfitting where significant d-sep tests are obscured by many more non-significant values leading to strong support for the (potentially incorrect) model structure. Paradoxically, poor sample size can also lead to a good-fitting model because the tests lack the power to detect an actual effect (high Type II error), leading to the paradoxical situation of a well-supported model whose paths are all non-significant. Such biases should be considered when reporting the results of the test, and the tolerance of error (i.e., \\(\\alpha\\)) could be adjusted for larger datasets. In this way, the tests of directed separation may be usefully diagnostic by drawing attention to the specific relationships that could be re-inserted into the model, which would have the added benefit of improving model fit by removing those significant P-values from the basis set. Whether this is advisable depends on the goal of the exercise: in an exploratory mode, for example, adding paths might be useful if they are theoretically justifiable. I would not, however, recommend selecting all non-significant paths and reinserting them into the model to improve model fit, or iteratively re-adding paths until adequate fit is achieved. Keep in mind that those paths were not included in the original path diagram because you, the user, did not consider them important. Why would you put them back into the model? Rather, perhaps it is the original model structure or the data that is inadequate, and you should consider alternative formulations, additional covariates, or other modifications to better reflect the reality suggested by the data. 3.3 A Log-Likelihood Approach to Assessing Model Fit Recently, Shipley and Douma developed an alternative index of model fit using maximum likelihood. You will recall from the chapter on global estimation that maximum likelihood estimation searches over parameter space for the values of the coefficients \\(\\theta\\) that maximize the probability \\(P\\) of having observed your data \\(X\\). The likelihood \\(L\\) of the model is therefore the value produced by this function when \\(P\\) is maximized by the model-estimated coefficients \\(\\hat{\\theta}\\): \\(L(\\hat{\\theta} | X)\\). This is often reported as the log-likelihood (the log of this value) as it simplifies the calculation of the fitting function for common statistical distributions: \\(log(L(\\hat{\\theta} | X))\\). Each component model in the SEM produces a log-likelihood, assuming it is fit using maximum likelihood estimation. The log-likelihood of the full set of structural equations \\(M\\) is therefore the sum of the individual log-likelihood values for each of the submodels (based on Markov decomposition of the joint probability distribution): \\[log(L_M(\\theta | X)) = {\\sum_{i=1}^k log(L_i(\\theta_i | X_i)}\\] This global quantity summarizes the likelihood associated with ALL of the coefficients across all models. Recall from the global estimation chapter that the objective there is to minimize the discrepancy between the model estimated and observed variance-covariance matrices. This is essentially testing whether the covariance between unlinked variables is zero. Take this model from that chapter: Here, the assumption is that the covariance between \\(x1\\) and \\(y2\\) is zero. In other words, by omitting a direct path between them, we are claiming that they are conditionally independent or causally unrelated. Deviations from this assumption will cause the model-estimated variance-covariance matrix to pull away from from the observed matrix, causing the \\(F_{ML}\\) to increase, the likelihood to decline, and the fit to become increasingly poor. In a piecewise context, we could rewrite the structural equation \\(y2 ~ y1\\) to be \\(y2 ~ y1 + 0*x1\\) to reflect our assumption that the coefficient for the relationship between \\(x1\\) and \\(y2\\) is zero. This is different from the equation \\(y2 ~ y1 + x1\\), which would produce an estimate for this relationship. In the d-sep tests, we actually fit this second model and extract the P-value associated with this estimate. If P&gt;0.05, we validate our original assumption that the two are independent, conditional on \\(y1\\). But it could just as easily not be zero, and we would reject the fit of the model. We can also produce a log-likelihood for this second model that allows \\(x1\\) to vary freely. A likelihood ratio test would then allow us to tell whether the second model was more or less likely within some tolerance for error. If the second model is more likely, they we were incorrect in assuming the relationship between \\(x1\\) and \\(y2\\) is zero, and our original formulation could be considered a poor representation of the data. Shipley and Douma showed that this procedure can be extended to the entire SEM, first by incorporating ALL missing paths (or the variables whose covariances we assume to be 0), then taking the difference in log-likelihoods for each submodel between the proposed model and the one with all paths accounted for, and finally summarizing these differences to get an overall index of model fit. How, then, to re-fit the models including the missing parameters? We already have a handy blueprint in the form of the basis set, which tells us which relationships are missing. We can reparameterize the models to include the missing paths specified in the basis set, extract the log-likelihood, and compare that value to the log-likelihood of the original models. Because this procedure would require all variables to be linked, this procedure is essentially fitting a fully saturated model (i.e., all variables are linked) and comparing it to a nested model where some paths are missing. Note that this test is not possible for models that are already saturated, which is also true for global estimation. If we assume the fully saturated model \\(M_2\\) and the proposed (nested) model \\(M_1\\), Shipley and Douma show that the \\(\\chi^2\\) statistic can be computed as follows: \\[\\chi^2_ML = -2(log(L(M_1)) - log(L(M_2))\\] Which can be compared to a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom, where \\(k\\) is the sum of the differences in the likelihood degrees of freedom (number of likelihood-estimated parameters) between each of the submodels in \\(M1\\) and \\(M2\\). If we allow the previously unestimated paths to freely vary, but this in turn does not improve the likelihood of the model, then this {^2} value becomes increasingly smaller. In this way, the procedure is the exact analogue of the classic \\(F_{ML}\\) in global estimation, we also seeks to minimize these differences between two matrices. As we shall see, they actually produce identical results when we assume normality. As AIC is simply the log-likelihood penalized for complexity, Shipley and Douma go on to show that a model-wide AIC value can be derived as the sum of the AIC values for each submodel i in \\(M1\\): \\[AIC_{M1} = {\\sum_{i=1}^v AIC_i}\\] Note that this is not the difference in AIC values between the proposed and saturated models. Additionally, a corrected \\(AIC_c\\) can be substituted for small sample sizes. Note that the AIC obtained from the d-sep tests cannot be compared with that derived from likelihood, and vice versa. What is useful about this method is that it brings the technique more in parity with variance-covariance based SEM, and solves several outstanding issues associated with the d-sep tests, namely: d-sep tests consider only changes in topology so it is only useful when comparing models that differ in their independence claims, whereas a log-likelihood approach considers all changes to the model above and beyond the DAG, such as changing the underlying distribution (which changes the maximum likelihood fitting function) or random effects. this method can be applied for all models fit using maximum likelihood, including truly non-linear approaches such as generalized additive models (GAMs). Note, however, that GAMs do not produce traditional linear coefficients (but rather fitted smoothing functions), so there are still downsides with respect to paramterizing the DAG and drawing inference. it omits issues associated with reporting P-values for mixed models where the denominator degrees of freedom are unclear (see: lme4). This method cannot be used when the conditions for maximum likelihood are not met, such as for regresson on distance matrices, and so in these cases, only test of directed separation are possible. Nevertheless, log-likelihood based {^2} is an incredibly useful addition and will likely become the default goodness-of-fit metric moving forward due to its flexibility and similarity to traditional methods. 3.4 Model fitting using piecewiseSEM Fitting a piecewise structural equation model is as simple as fitting each regression separately: if you can fit an lm in R, you have already fit a SEM! The package of course is piecewiseSEM: library(piecewiseSEM) And lets return to the data from Grace &amp; Keeley (2006) that we explored in the chapter on global estimation: data(keeley) As a reminder, Grace &amp; Keeley wanted to understand patterns in plant diversity following disturbance, in this case wildfires in California. In the end of the global estimation chapter, we tested for full mediation using the following model: As in lavaan, its first necessary to break down the list of structural equations. Unlike lavaan these are not coded as character strings, but instead as full-fledged linear models. You can see where coding each model separately will impart greater flexibility, for example, by fitting a GLM, mixed-effects model, and so on. Rather than using the function sem, the list of models are put together using the function psem which is the primary workhouse of the piecewiseSEM package. keeley_psem &lt;- psem( lm(cover ~ firesev, data = keeley), lm(firesev ~ age, data = keeley), data = keeley) Note: Its not necessary to pass a data argument to psem but it can help alleviate errors in certain cases. Before we get to the model fitting, lets just examine the psem object by itself: keeley_psem ## Structural Equations of x : ## lm: cover ~ firesev ## lm: firesev ~ age ## ## Data: ## distance elev abiotic age hetero firesev cover rich ## 1 53.40900 1225 60.67103 40 0.757065 3.50 1.0387974 51 ## 2 37.03745 60 40.94291 25 0.491340 4.05 0.4775924 31 ## 3 53.69565 200 50.98805 15 0.844485 2.60 0.9489357 71 ## 4 53.69565 200 61.15633 15 0.690847 2.90 1.1949002 64 ## 5 51.95985 970 46.66807 23 0.545628 4.30 1.2981890 68 ## 6 51.95985 970 39.82357 24 0.652895 4.00 1.1734866 34 ## ...with 84 more rows ## ## [1] &quot;class(psem)&quot; It returns the submodels, their classes (in this case lm), and a snippet of the data. The first step is to derive the basis set using the function basisSet: basisSet(keeley_psem) ## $`1` ## [1] &quot;age | cover ( firesev )&quot; Here, there is a single independence claim representing the missing path from \\(age -&gt; cover\\) conditional on the influence of \\(firesev\\) on \\(cover\\). Now to evaluate the tests of directed separation using the function dSep: dSep(keeley_psem, .progressBar = FALSE) ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 cover ~ age + ... coef 87 -1.80184 0.07503437 Note that the output is the same as if we evaluated the independence claim ourselves: summary(lm(cover ~ firesev + age, data = keeley))$coefficients[3, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## -0.004832969 0.002682241 -1.801839905 0.075034374 Now, we can compute the Fishers C statistic using the P-value obtained from the d-sep test. Recall that the degrees of freedom for the test is twice the number of independence claims, so in this case \\(2*1 = 2\\). We can use these values to compare the statistic to a \\(\\chi^2\\) distribution to get a model-wide P-value: (C &lt;- -2 * log(summary(lm(cover ~ firesev + age, data = keeley))$coefficients[3, 4])) ## [1] 5.179618 1-pchisq(C, 2) ## [1] 0.07503437 So in this case, we would fail to reject the model as P&gt;0.05. Note that in the case of a single independence claim, the model-wide P-value is the same as the P-value for the individual claim. Alternatively, we could just use the function fisherC which constructs the statistic for us: fisherC(keeley_psem) ## Fisher.C df P.Value ## 1 5.18 2 0.075 Lets now compute the log-likelihood based \\(\\chi^2\\) statistic to see if we get the same answer. To do so, we must first create the saturated model, which in this case would involve fitting a path between \\(age\\) and \\(cover\\): keeley_psem2 &lt;- psem( lm(cover ~ firesev + age, data = keeley), lm(firesev ~ age, data = keeley), data = keeley ) From here, we can get the log-likelihoods of both submodels using the logLik function for each SEM, take their difference, and construct the \\(\\chi^2\\) statistic: LL_1 &lt;- logLik(lm(cover ~ firesev, data = keeley)) - logLik(lm(cover ~ firesev + age, data = keeley)) LL_2 &lt;- logLik(lm(firesev ~ age, data = keeley)) - logLik(lm(firesev ~ age, data = keeley)) (ChiSq &lt;- -2*sum(as.numeric(LL_1), as.numeric(LL_2))) ## [1] 3.297429 DF &lt;- 1 # one additional parameter estimated in the saturated model 1 - pchisq(ChiSq, DF) ## [1] 0.06938839 So we would also fail to reject the model based on the P-value obtained from this test. We can more easily obtain the same output using the function LLchisq on the original (unsaturated) model: LLchisq(keeley_psem) ## Chisq df P.Value ## 1 3.297 1 0.069 Note for models assuming multivariate normality (as we have here), the {^2} statistic and P-value are actually the same as we obtain from lavaan: library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. keeley_formula &lt;- &#39; firesev ~ age cover ~ firesev &#39; keeley_sem &lt;- sem(keeley_formula, data = keeley) fit &lt;- lavInspect(keeley_sem, &quot;fit&quot;) fit[&quot;chisq&quot;]; fit[&quot;pvalue&quot;] ## chisq ## 3.297429 ## pvalue ## 0.06938839 Note that we can also obtain an AIC score for the model. The default is based on the log-likelihood \\(\\chi^2\\) and we shall see why in a minute: AIC(keeley_psem) ## AIC K n ## 1 364.696 6 90 To get the AIC value based on the Fishers C statistic and the d-sep tests, we can add the following argument: AIC(keeley_psem, AIC.type = &quot;dsep&quot;) ## AIC K n ## 1 17.18 6 90 Ah, a fully saturated or just identified model will yield a C statistic of 0. Based on Shipleys equation above, the AIC score reduces to \\(2K\\), or twice the likelihood degrees of freedom. This is in contrast to alternative formulation, which is based on actual likelihoods. Therefore, in situations where one wishes to compare a model that is fully saturated, we advise using the default \\(\\chi^2\\)-based value. This exercise was a long workaround to reveal that all the above can be executed simultaneously using the summary function on the SEM object: summary(keeley_psem, .progressBar = FALSE) ## ## Structural Equation Model of keeley_psem ## ## Call: ## cover ~ firesev ## firesev ~ age ## ## AIC ## 364.696 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## cover ~ age + ... coef 87 -1.8018 0.075 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.297 with P-value = 0.069 and on 1 degrees of freedom ## Fisher&#39;s C = 5.18 with P-value = 0.075 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## cover firesev -0.0839 0.0184 88 -4.5594 0 -0.4371 *** ## firesev age 0.0597 0.0125 88 4.7781 0 0.4539 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## cover none 0.19 ## firesev none 0.21 The output should look very familar to the output from other summary calls, like summary.lm. The d-sep tests, \\(\\chi^2\\) and Fishers C tests of goodness-of-fit, and AIC are all reported. Additionally, model coefficients are returned. Unlike lavaan, the standardized estimates are provided by default. Also unlike lavaan, the individual model \\(R^2\\) values are also returned by default. Both sets of statistics are key for inference, and thus we have decided to make them available with any further arguments passed to summary. We can compare the piecewiseSEM output to the lavaan output: library(lavaan) sem1 &lt;- &#39; firesev ~ age cover ~ firesev &#39; keeley_sem1 &lt;- sem(sem1, keeley) summary(keeley_sem1, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 3.297 ## Degrees of freedom 1 ## P-value (Chi-square) 0.069 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.060 0.012 4.832 0.000 0.060 0.454 ## cover ~ ## firesev -0.084 0.018 -4.611 0.000 -0.084 -0.437 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 2.144 0.320 6.708 0.000 2.144 0.794 ## .cover 0.081 0.012 6.708 0.000 0.081 0.809 ## ## R-Square: ## Estimate ## firesev 0.206 ## cover 0.191 Again, because we are making the same assumptions as for global estimation (i.e., multivariate normality), all of the output will be identical (or nearly identical based on rounding and optimization differences). Of course, we might expect greater divergence between the two methods if we were to incorporate different distributions and more complex model structures, which we will explore now. 3.5 Extensions to Generalized Mixed Effects Models Lets turn to the example from Shipley (2009) on tree survival. In this (hypothetical) study, individual trees are followed for 36 years at 20 sites and measured for date of bud burst (Date), cumulative degree days until first bud burst (DD), growth, and survival. Its important to note that these data have multiple levels of hierarchical structure: between sites, between individuals within sites, between years within individuals within sites. They also have non-normal responses: survival is measured as a binary outcome (alive or dead). Shipley hypothesized these variables are related in the following way: Lets first treat the data as normal and independent using lavaan: data(shipley) shipley_model &lt;- &#39; DD ~ lat Date ~ DD Growth ~ Date Live ~ Growth &#39; shipley_sem &lt;- sem(shipley_model, shipley) summary(shipley_sem, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 27 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Used Total ## Number of observations 1431 1900 ## ## Model Test User Model: ## ## Test statistic 38.433 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## DD ~ ## lat -0.860 0.023 -37.923 0.000 -0.860 -0.708 ## Date ~ ## DD -0.517 0.016 -32.525 0.000 -0.517 -0.652 ## Growth ~ ## Date 0.173 0.020 8.508 0.000 0.173 0.219 ## Live ~ ## Growth 0.006 0.001 9.854 0.000 0.006 0.252 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .DD 52.628 1.967 26.749 0.000 52.628 0.499 ## .Date 38.080 1.424 26.749 0.000 38.080 0.575 ## .Growth 38.981 1.457 26.749 0.000 38.981 0.952 ## .Live 0.025 0.001 26.749 0.000 0.025 0.936 ## ## R-Square: ## Estimate ## DD 0.501 ## Date 0.425 ## Growth 0.048 ## Live 0.064 First, we notice the goodness-of-fit can be estimated, but the model is a poor fit (P&lt;0.001). The paths are all significant but this doesnt do us much good considering the model is not suitable for inference. Instead of fiddling with modification indices and trying to rejigger the model strcuture, lets analyze the same path diagram using a piecewise approach and recognizing both the hierarchical structure AND non-normality of the data. For this we will use two common packages for mixed-effects models, lme4 and nlme: library(nlme) library(lme4) shipley_psem &lt;- psem( lme(DD ~ lat, random = ~ 1 | site / tree, na.action = na.omit, data = shipley), lme(Date ~ DD, random = ~ 1 | site / tree, na.action = na.omit, data = shipley), lme(Growth ~ Date, random = ~ 1 | site / tree, na.action = na.omit, data = shipley), glmer(Live ~ Growth + (1 | site) + (1 | tree), family = binomial(link = &quot;logit&quot;), data = shipley) ) summary(shipley_psem, .progressBar = FALSE) ## ## Structural Equation Model of shipley_psem ## ## Call: ## DD ~ lat ## Date ~ DD ## Growth ~ Date ## Live ~ Growth ## ## AIC ## 21745.782 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## Date ~ lat + ... coef 18 -0.0798 0.9373 ## Growth ~ lat + ... coef 18 -0.8929 0.3837 ## Live ~ lat + ... coef 1431 1.0280 0.3039 ## Growth ~ DD + ... coef 1329 -0.2967 0.7667 ## Live ~ DD + ... coef 1431 1.0046 0.3151 ## Live ~ Date + ... coef 1431 -1.5617 0.1184 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = NA with P-value = NA and on 6 degrees of freedom ## Fisher&#39;s C = 11.536 with P-value = 0.484 and on 12 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## DD lat -0.8355 0.1194 18 -6.9960 0 -0.6877 *** ## Date DD -0.4976 0.0049 1330 -100.8757 0 -0.6281 *** ## Growth Date 0.3007 0.0266 1330 11.2917 0 0.3824 *** ## Live Growth 0.3479 0.0584 1431 5.9552 0 0.7866 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method Marginal Conditional ## DD none 0.49 0.70 ## Date none 0.41 0.98 ## Growth none 0.11 0.84 ## Live delta 0.16 0.18 The immediately obvious difference based on Fishers C is that the model is no longer a poor fit: we have 12 degrees of freedom corresponding to 6 independence claims, all of which have P &gt; 0.05. Therefore, the model-wide P = 0.484, and we would therefore reject the null that the data do not support the hypothesized model structure. Moreover, while the direction of the parameter estimates remain the same, they vary considerably in their magnitudes (e.g., \\(\\beta_{date, DD} = -0.652\\) for lavaan and \\(\\beta_{date, DD} = -0.497\\) in piecewiseSEM). The model \\(R^2\\)s are all higher as well, for fixed-effects only (marginal) and especially for fixed- and random-effects together (conditional). Thus, by addressing the non-independence of the data, we have converged on support for the hypothesized model structure, more accurate parameter estimates, and a higher proportion of explained variance than was possible using lavaan. Note, however, that the model does not report a \\(\\chi^2\\) statistic, and issues a warning about convergence. When one or more of submodels in the saturated SEM fail to converge, it may produce invalid likelihood estimates that lead to the situation where \\(\\chi^2\\)&lt;0. Since this is not permissible, the function returns NA for the \\(\\chi^2\\) statistic and associated P-value. Potential solutions include tweaking model optimizers to ensure a convergent solution, or relying on Fishers C. 3.6 Extensions to Non-linear Models One of the benefits noted above for the newer log-likelihood based goodness-of-fit procedure is that it can be extended to truly non-linear models, such as generalized additive models (GAMs), where maximum likelihood estimation is applied. Lets now work through an example extending to GAMs. Well work from the random dataset generated in the supplements for the paper by Shipley and Douma. First, lets generate the data using their code: set.seed(100) n &lt;- 100 x1 &lt;- rchisq(n, 7) mu2 &lt;- 10*x1/(5 + x1) x2 &lt;- rnorm(n, mu2, 1) x2[x2 &lt;= 0] &lt;- 0.1 x3 &lt;- rpois(n, lambda = (0.5*x2)) x4 &lt;- rpois(n, lambda = (0.5*x2)) p.x5 &lt;- exp(-0.5*x3 + 0.5*x4)/(1 + exp(-0.5*x3 + 0.5*x4)) x5 &lt;- rbinom(n, size = 1, prob = p.x5) dat2 &lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5) Youll note that there is a mix of linear and non-linear variables, including Poisson- and binomial-distributions. Now lets consider the SEM from their paper (Figure 1a): In the paper, Shipley and Douma first fit a strictly linear SEM assuming multivariate normality, which we will also do here: shipley_psem2 &lt;- psem( lm(x2 ~ x1, data = dat2), lm(x3 ~ x2, data = dat2), lm(x4 ~ x2, data = dat2), lm(x5 ~ x3 + x4, data = dat2) ) LLchisq(shipley_psem2) ## Chisq df P.Value ## 1 4.143 5 0.529 We see that, despite having generated data that is inherently non-normal and non-linear, the model is actually a good fit to the data with a P-value of 0.529. Nevertheless, we know we can do better, both by including non-Gaussian distributions and also through the application of generalized additive models, which model the response not as a linear function of the predictors, but through smoothing functions that allow for non-linear relationships to emerge. Lets re-fit the model using a mix of GLMs and GAMs: library(mgcv) ## This is mgcv 1.8-36. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. shipley_psem3 &lt;- psem( gam(x2 ~ s(x1), data = dat2, family = gaussian), glm(x3 ~ x2, data = dat2, family = poisson), gam(x4 ~ x2, data = dat2, family = poisson), glm(x5 ~ x3 + x4, data = dat2, family = binomial) ) LLchisq(shipley_psem3) ## Chisq df P.Value ## 1 3.346 5 0.647 This model also has adequate fit, with a P-value of 0.647. Note that these are the same values reported in Table 2. How to choose among them? Lets compute the AIC scores (based on log-likelihoods) for both and compare them: AIC(shipley_psem2, shipley_psem3) ## AIC K n ## 1 1240.20 13.000 100 ## 2 1190.75 11.563 100 We see that the second SEMthe one that better addresses the underlying forms of the datahas much higher support than the straight linear SEM, with the \\(\\Delta AIC = 49.45\\). Thus, we would far and away choose the second model, in line with how Shipley and Douma have designed their simulation. Now lets examine the summary output for this second model: summary(shipley_psem3) ## Warning: Categorical or non-linear variables detected. Please refer to ## documentation for interpretation of Estimates! ## ## Structural Equation Model of shipley_psem3 ## ## Call: ## x2 ~ s(x1) ## x3 ~ x2 ## x4 ~ x2 ## x5 ~ x3 + x4 ## ## AIC ## 1190.750 ## ## --- ## Tests of directed separation: ## ## No independence claims present. Tests of directed separation not possible. ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.346 with P-value = 0.647 and on 5 degrees of freedom ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## x2 s(x1) - - 3.2242 41.2590 0e+00 - ## x3 x2 0.1464 0.0373 98.0000 3.9248 1e-04 0.363 ## x4 x2 0.1741 0.038 100.0000 4.5856 0e+00 0.3565 ## x5 x3 -0.5083 0.1521 97.0000 -3.3414 8e-04 -0.4037 ## x5 x4 0.4587 0.1388 97.0000 3.3047 1e-03 0.4414 ## ## *** ## *** ## *** ## *** ## *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## x2 none 0.57 ## x3 nagelkerke 0.21 ## x4 none 0.14 ## x5 nagelkerke 0.28 Most of this should look familiar. Note that the output does not report estimates or standard errors for the smoothed variables. This is because, as noted previously, these are not linear coefficients but instead smoothing functions and there is not a single value relating how, for example, \\(x1\\) changes with \\(x2\\). Therefore, calculation of direct and indirect effects is not possible using GAMs, which could make them less than ideal when the goal is to compare the strength of various pathways within a model. However, it is possible, as we have shown here, to compare among models and validate the topology of the DAG, which in many cases is sufficient to test hypotheses. Those seeking to understand how \\(x1\\) changes non-linearly with \\(x2\\) would then have to present additional predicted plots of this relationship in addition to the path diagram. 3.7 A Special Case: Where Graph Theory Fails In the majority of cases, as we have established, the direction of the independence claim doesnt matter because, while the coefficients will differ, their P-values will be identical. Thus it doesnt matter if you test \\(y | x\\) or \\(x | y\\) because the claim will yield the same significance test. EXCEPT when intermediate endogenous variables are non-normally distributed. Consider the following SEM: In this SEM, there are two independence claims: \\(y3 | x1 (y1, y2)\\) \\(y2 | y1 (x1)\\) In the second independence claim, if both variables were normally distributed, the significance value is the same whether the test is conducted as \\(y2 | y1 (x1)\\) or \\(y1 | y2 (x1)\\). This is NOT true, however, when one or both of the responses are fit to a non-normal distribution. This is because the response is now transformed via a link function \\(g(\\mu)\\) (see chapter on coefficients), and the parameter estimatesand their standard errorsare now expressed on the link scale. This transformation means the P-value obtained by regressing \\(y1 ~ y2\\) is NOT the same as the one obtained by regressing \\(y2 ~ y1\\). To show this is true, lets generate some Poisson-distributed data and model using both LM and GLM with a log-link: set.seed(87) glmdat &lt;- data.frame(x1 = runif(50), y1 = rpois(50, 10), y2 = rpois(50, 50), y3 = runif(50)) # LM summary(lm(y1 ~ y2 + x1, glmdat))$coefficients[2, 4] ## [1] 0.03377718 summary(lm(y2 ~ y1 + x1, glmdat))$coefficients[2, 4] ## [1] 0.03377718 # GLM summary(glm(y1 ~ y2 + x1, &quot;poisson&quot;, glmdat))$coefficients[2, 4] ## [1] 0.03479666 summary(glm(y2 ~ y1 + x1, &quot;poisson&quot;, glmdat))$coefficients[2, 4] ## [1] 0.08586767 In the case of lm the P-value is identical regardless of the direction, and moreover is &lt; 0.05, thusdepending on the outcome of the other claimwe might reject the model. In contrast, when \\(y1\\) and \\(y2\\) are modeled as Poisson-distributed, the P-value is alternatingly &lt; and &gt;= 0.05. Thus, depending on how the claim is specified, we might or might not reject the model. A big difference! Note that the log-likelihoods are also different for GLM: logLik(glm(y1 ~ y2 + x1, &quot;poisson&quot;, glmdat)) ## &#39;log Lik.&#39; -128.8009 (df=3) logLik(glm(y2 ~ y1 + x1, &quot;poisson&quot;, glmdat)) ## &#39;log Lik.&#39; -158.0841 (df=3) piecewiseSEM solves this by providing three options to the user. We can specify the directionality of the test if, for instance, it makes greater biological sense to test \\(y1\\) against \\(y2\\) instead of the reverse (for example: abundance drives species richness, not vice versa); or We can remove that path from the basis set and instead specify it as a correlated error using %~~%. This circumvents the issue altogether but it may not make sense to assume both variables are generated by some underlying process; or We can conduct both tests and choose the most conservative (i.e., lowest) P-value or maximum difference in the log-likelihoods. These options are returned by summary in the event the above scenario is identified in the SEM: glmsem &lt;- psem( glm(y1 ~ x1, &quot;poisson&quot;, glmdat), glm(y2 ~ x1, &quot;poisson&quot;, glmdat), lm(y3 ~ y1 + y2, glmdat) ) summary(glmsem) ## Error: ## Non-linearities detected in the basis set where P-values are not symmetrical. ## This can bias the outcome of the tests of directed separation. ## ## Offending independence claims: ## y2 &lt;- y1 *OR* y2 -&gt; y1 ## ## Option 1: Specify directionality using argument &#39;direction = c()&#39; in &#39;summary&#39;. ## ## Option 2: Remove path from the basis set by specifying as a correlated error using &#39;%~~%&#39; in &#39;psem&#39;. ## ## Option 3 (recommended): Use argument &#39;conserve = TRUE&#39; in &#39;summary&#39; to compute both tests, and return the most conservative P-value. In option 1, the directionality can be specified using direction = c() as an additional argument to summary. summary(glmsem, direction = c(&quot;y1 &lt;- y2&quot;), .progressBar = F)$dTable ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 y3 ~ x1 + ... coef 46 -0.7187 0.4760 ## 2 y2 ~ y1 + ... coef 47 1.7176 0.0859 In option 2, the SEM can be updated to remove that test by specifying it as a correlated error. summary(update(glmsem, y1 %~~% y2), .progressBar = F) ## ## Structural Equation Model of update(glmsem, y1 %~~% y2) ## ## Call: ## y1 ~ x1 ## y2 ~ x1 ## y3 ~ y1 + y2 ## y1 ~~ y2 ## ## AIC ## 609.236 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## y3 ~ x1 + ... coef 46 -0.7187 0.476 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.558 with P-value = 0.455 and on 1 degrees of freedom ## Fisher&#39;s C = 1.485 with P-value = 0.476 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## y1 x1 -0.1007 0.1573 48 -0.6402 0.5221 -0.0895 ## y2 x1 0.0252 0.0737 48 0.3423 0.7322 0.0607 ## y3 y1 -0.0160 0.0128 47 -1.2511 0.2171 -0.1830 ## y3 y2 0.0144 0.0074 47 1.9416 0.0582 0.2839 ## ~~y1 ~~y2 0.3155 - 50 2.2792 0.0136 0.3155 * ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## y1 nagelkerke 0.01 ## y2 nagelkerke 0.00 ## y3 none 0.08 Note that the claim no longer appears in the section for the tests of directed separation. Finally, option 3 can be invoked by specifying conserve = T as an additional argument summary(glmsem, conserve = T, .progressBar = F)$dTable ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 y3 ~ x1 + ... coef 46 -0.7187 0.4760 ## 3 y1 ~ y2 + ... coef 47 2.1107 0.0348 * The user should be vigilant for these kinds of situations and ensure that both the specified paths AND the independence claims all make biological sense. In the case where the underlying assumptions of the d-sep tests can bias the goodness-of-fit statistic, piecewiseSEM should automatically alert the user and suggest solutions. 3.8 References Shipley, Bill. A new inferential test for path models based on directed acyclic graphs. Structural Equation Modeling 7.2 (2000): 206-218. Shipley, Bill. Confirmatory path analysis in a generalized multilevel context. Ecology 90.2 (2009): 363-368. Shipley, Bill. The AIC model selection method applied to path analytic models compared using adseparation test. Ecology 94.3 (2013): 560-564. Lefcheck, Jonathan S. piecewiseSEM: Piecewise structural equation modelling in r for ecology, evolution, and systematics. Methods in Ecology and Evolution 7.5 (2016): 573-579. Shipley, Bill, and Jacob C. Douma. Generalized AIC and chisquared statistics for path models consistent with directed acyclic graphs. Ecology 101.3 (2020): e02960. "],["coefficients.html", "4 Coefficients 4.1 Unstandardized and Standardized Coefficients 4.2 Scale Standardization 4.3 Range Standardization 4.4 Binomial Response Models 4.5 Scaling to Other Non-Normal Distributions 4.6 References", " 4 Coefficients 4.1 Unstandardized and Standardized Coefficients Path (or regression) coefficients are the inferential engine behind structural equation modeling, and by extension all of linear regression. They relate changes in the dependent variable \\(y\\) to changes in the independent variable \\(x\\), and thus act as a measure of association. In fact, you may recall from the chapter on global estimation that, under some circumstances, path coefficients can be expressed as (partial) correlations, a unitless measure of association that makes them excellent for comparisons. They also allow us to generate predictions for new values of \\(x\\) and are therefore useful in testing and extrapolating model results. We will consider two kinds of regression coefficients: unstandardized (or raw) coefficients, and standardized coefficients. Unstandardized coefficients are the default values returned by all statistical programs. In short, they reflect the expected (linear) change in the response with each unit change in the predictor. For a coefficient value \\(\\beta = 0.5\\), for example, a 1 unit change in \\(x\\) is, on average, an 0.5 unit change in \\(y\\). In models with more than one independent variable (e.g., \\(x1\\), \\(x2\\), etc), the coefficient reflects the expected change in \\(y\\) given the other variables in the model. This implies that the effect of one particular variable controls for the presence of other variables, generally by holding them constant at their mean. This is why such coefficients are referred to as partial regression coefficients, because they reflect the independent (or partial) contributions of any particular variable. As an aside: one tricky aspect to interpretation involves transformations. When the log-transformation is applied, for example, the relationships between the variable are no longer linear. This means that we have to change our interpretation slightly. When \\(y\\) is log-transformed, the coefficient \\(\\beta\\) is interpreted as a 1 unit change in \\(x\\) leads to a \\((exp(\\beta) - 1) \\times 100%\\) change in \\(y\\). Oppositely, when the independent variable \\(x\\) is log-transformed, \\(\\beta\\) is interpreted as a 1% change in \\(x\\) leads to a \\(\\beta\\) change in \\(y\\). Finally, when both are transformed, both are expressed in percentages: a 1% change in \\(x\\) leads to a \\((exp(\\beta) - 1) \\times 100%\\) change in \\(y\\). Transformations often confound interpretation, so it is worth mentioning. In contrast to raw coefficients, standardized coefficients are expressed in equivalent units, regardless of the original measurements. Often these are in units of standard deviations of the mean (scale standardization) but, as we shall see shortly, there are other possibilities. The goal of standardization is to increase comparability. In other words, the magnitude of standardized coefficients can be directly compared to make inferences about the relative strength of relationships. In SEM, it is often advised to report both unstandardized and standardized coefficients, because they present different and mutually exclusive information. Unstandardized coefficients contain information about both the variance and the mean, and thus are essential for prediction. Along these lines, they are also useful for comparing across models fit to the same variables, but using different sets of data. Because the most common form of standardization involves scaling by the sample standard deviations, data derived from different sources (i.e., different datasets) have different sample variances and their standardized coefficients are not immediately comparable. Unstandardized coefficients also reflect the phenomenon of interest in straightforward language. Imagine telling someone that 1 standard deviation change in nutrient input levels would result in a 6 standard deviation change in water quality. That might seem impressive until it becomes clear that the size of the dataset has reduced the sample variance, and the absolute relationship reveals only a very tiny change in water quality with each unit change in nutrient levels. Not so impressive anymore. Standardized effects, on the other hand, are useful for comparing the relative magnitude of change associated with different paths in the same model (i.e., using data drawn from the same population). Care should be taken not to interpret these relationships as the proportion of variance explainedfor example, a larger standardized coefficient does not explain more variance in the response than a smaller standardized coefficientbut rather in terms of relative influence on the mean of the response. By extension, standardization is necessary to compare indirect or compound effects among different sets of paths in the same model: for example, comparing direct vs. indirect pathways in a partial mediation model. This is because those pathways can and often are measured in very different units, and their relative magnitudes might simply reflect their measurement units rather than any stronger or weaker effects. In contrast, comparing the strength of indirect or compound effects across the same set of variables in different models requires unstandardized coefficients, due to the issue of different sample variances raised above. Comparing the same path across different models using standardized coefficients would require a demonstration that the sample variances are not significantly different (or alternately, that the entire population has been sampled). Thus, both standardized and unstandardized coefficients have their place in structural equation modeling. Lets now explore some of the different forms of standardization, and how they can be achieved. 4.2 Scale Standardization The most typical implementation of standardization is placing the coefficients in units of standard deviations of the mean. This is accomplished by scaling the coefficient \\(\\beta\\) by the ratio of the standard deviation of \\(x\\) over the standard deviation of \\(y\\): \\[b = \\beta*\\left( \\frac{sd_x}{sd_y} \\right)\\] This coefficient has the following interpretation: for a 1 standard deviation change in \\(x\\), we expect a \\(b\\) unit standard deviation change in \\(y\\). This standardization can also be achieved by Z-transforming the raw data, in which case \\(b\\) is already the (partial) correlation between \\(x\\) and \\(y\\). Both lavaan and piecewiseSEM return scale-standardized coefficients. lavaan requires a different set of functions or arguments, while piecewiseSEM will do it by default using the functions coefs. coefs has the added benefit in that it can be called on any model object, and thus has applications outside of structural equation modeling. Lets run an example: library(lavaan) library(piecewiseSEM) set.seed(6) data &lt;- data.frame(y = runif(100), x = runif(100)) xy_model &lt;- lm(y ~ x, data = data) # perform manual standardization beta &lt;- summary(xy_model)$coefficients[2, 1] (beta_std &lt;- beta * (sd(data$x)/sd(data$y))) ## [1] 0.09456659 For this example, we recover a standardized coefficient of \\(b = 0.095\\), suggesting that for a 1 standard deviation change in \\(x\\), there is expectde to be a 0.095 standard deviation change in \\(y\\). # now retrieve with piecewiseSEM coefs(xy_model)$Std.Estimate ## [1] 0.0946 We get the same estimate from piecewiseSEM using coefs. # and with lavaan xy_formula &lt;- &#39;y ~ x&#39; xy_sem &lt;- sem(xy_formula, data) standardizedsolution(xy_sem)$est.std[1] ## [1] 0.09456659 And the same for lavaan, demonstrating that these packages use the same scaling procedure under the hood. 4.3 Range Standardization An alternative to scale standardization is relevant range standardization. This approach scales the coefficients over some relevant range. Typically this is the full range of the data, in which case \\(\\beta\\) can be standardized as follows: \\[b = \\beta * \\frac{max(x) - min(x)}{max(y) - min(y)}\\] The interpretation for the coefficient would then be the expected proportional shift in \\(y\\) along its range given a full shift along the range of \\(x\\). At first, this might seem like a strange form of standardization, but it has some powerful applications. For example, consider a binary predictor: 0 or 1. In such a case, the relevant range-standardized coefficient is the expected shift in \\(y\\) given the transition from one state (0) to another (1). Or consider a management target such as decreasing nutrient runoff by 10%. Would reducing fertilizer application by 10% of its range yield a similar reduction in runoff? Such expressions are necessarily the currency of applied questions. Perhaps the best application of relevant ranges is in comparing coefficients within a model: rather than dealing in somewhat esoteric quantities of standard deviations, relevant range standardization simply asks which variable causes a greater shift in \\(y\\) along its range. This is a much more digestable concept to most scientists. It may even provide a more fair comparison across the same paths fit to different datasets, if the ranges are roughly similar and/or encompassed in the others. Restricting the range may be a useful solution for comparing coefficients across models fit to different data, as long as the range doesnt extend beyond that observed in any particular dataset. Note that the decomposition of (partial) correlations as shown in the chapter on global estimation is not possible with relevant ranges, so range standardization is not recommended if the objective to compute indirect or total effects. For a worked example, we have now entered fully into the realm of piecewiseSEMit does not appear as if lavaan has integrated this functionality a of yet. Lets attempt to scale the results by hand, then compare to the output from coefs with the argument standardize = \"range\": #by hand (beta_rr &lt;- beta * (max(data$x) - min(data$x))/(max(data$y) - min(data$y))) ## [1] 0.09806703 coefs(xy_model, standardize = &quot;range&quot;)$Std.Estimate ## [1] 0.0981 In both cases, we obtain a \\(b = 0.098\\) suggesting that a full shift in \\(x\\) along its range would only result in a predicted shift of about 10% along the range of \\(y\\). Both scale and relevant range-standardization only apply when the response is normally-distributed. If not, we must make some assumptions in order to obtain standardized coefficients. Lets start with binomial responses, which are the trickiest case. 4.4 Binomial Response Models Binomial responses are those that are binary (0, 1) such as success or failure, or present vs. absent. What is unique about them is that they do not have a linear relationship with a predictor \\(x\\). Instead, they are best modeled using a sigmoidal curve. To demonstrate, lets generate some data, fit a binary model, and plot the predicted relationship: set.seed(44) x &lt;- rnorm(20) x &lt;- x[order(x)] y &lt;- c(rbinom(10, 1, 0.8), rbinom(10, 1, 0.2)) glm_model &lt;- glm(y ~ x, data = data.frame(x = x, y = y), &quot;binomial&quot;) xpred &lt;- seq(min(x), max(x), 0.01) ypred &lt;- predict(glm_model, list(x = xpred), type = &quot;response&quot;) plot(x, y) lines(xpred, ypred) Clearly these data are not linear, and modeling them as such would ignore the underlying data-generating process. Instead, as you can see, we fit them to a binomial distribution using a generalized linear model (GLM). GLMs consist of three parts: (1) the random component, or the expected values of the response based on their underlying distribution, (2) the systematic component that represents the linear combination of predictors, and (3) the link function, which links the expected values of the response (random component) to the linear combination of predictors (systematic component) via a transformation. Basically, the link functions take something inherently non-linear and attempts to linearize it. This can be shown by plotting the predictions on the link-scale: ypred_link &lt;- predict(glm_model, list(x = xpred), type = &quot;link&quot;) plot(xpred, ypred_link) Note how the line is no longer sigmoidal, but straight! For binomial responses, there are two kinds of link functions: logit and probit. Well focus on the logit link for now because its more common. With this link, the coefficients are in units of logits or the log odds ratio, which reflect the log of the probability of observing an outcome (1) relative to the probability of not observing it (0). Often these coefficients are reverted to just the odds ratio by taking the exponent, which yields the proportional change in the probablity observing one outcome (1) with a unit change change in the predictor. Say, for example, we have a coefficient \\(\\beta = -0.12\\). A 1 unit change in \\(x\\) would result in \\(exp(-0.12) = 0.88 \\times 100%\\) or 88% reduction in the odds of observing the outcome \\(y\\). The problem is that (log) odds ratios themselves are not comparable across models. Further, its not immediately clear how they might be standardized since the coefficient is reported on the link (linear) scale, while the only variance we can compute is from the raw data, which is on the non-linear scale. Thus, we need to find some sway to obtain estimates of variance on the same linearized scale as the coefficient. 4.4.1 Latent Theoretic Approach One approach is to consider that for every value of \\(x\\), there is an underlying probability distribution of observing a 0 or a 1 for \\(y\\). The mean of these distributions is where a particular outcome is most likely. Lets say at low values of \\(x\\) we observe \\(y = 0\\), at at high values of \\(x\\) we observe \\(y = 1\\). If we order \\(x\\), the mean probabilities give rise to a linear increase in observing \\(y = 1\\) with increasing \\(x\\). Here is an illustration of this phenomenon (from Long 1997): This linear but latent (i.e., unobserved) variable, which we call \\(y^*\\), is therefore related to the observed values of \\(x\\) through a vector of linear coefficients \\(\\beta\\) as in any other linear model: \\[y^*_{i} = x_{i}\\beta + \\epsilon_{i}\\] Generally, the linear \\(y^*\\) is related to the non-linear \\(y\\) via a cutpoint, which is generally \\(\\tau = 0.5\\) where any value of \\(x\\) where \\(y^*\\)&gt;0.5 is equivalent to \\(y\\) = 1, and any value of \\(x\\) where \\(y^*\\)&lt;0.5 is equivalent to \\(y\\) = 0. The problem is we can never observe this linear underlying or latent propensity and so we must approximate its error variance. In a later chapter on Latent Variable Modeling, we often fixed their error variance to 1. In this case, there are theoretically-derived error variances depending on the distribution and the link function: for the probit link, the error variance \\(\\epsilon = 1\\), while for the logit link, \\(\\epsilon = \\pi^2/3\\), both for the binomial distribution. Regardless of the type of standardization, we need to know about the range or variance of the response. With our knowledge of \\(y^*_{i}\\) and the theoretical error variances, we have all the information needed to compute the variance on the link (linear) scale. The variance in \\(y^*\\) is the sum of the variance of the linear (link-transformed) predictions plus the theoretical error variance. For a logit link, then: \\[\\sigma_{y^*_{i}}^2 = \\sigma_{x\\beta}^2 + \\pi^2/3\\] The square-root of this quantity gives the standard deviation of \\(SD_{y^*}\\) on the linear scale for use in scale standardization, or alternately, the range of \\(y^*\\) to use in relevant range standardization. 4.4.2 Observation-Empirical Approach There is an alternate method which relies on the proportion of variance explained, or \\(R^2\\). Here, we can express the \\(R^2\\) as the ratio of the variance of the predicted values (on the linear scale) over the variance of the observed values (on the non-linear scale): \\[R^2 = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_{y}^2}\\] We can compute \\(R\\), which is the correlation between the non-linear observed and predicted values on the non-linear scale that, when taken to the power of 2, yields \\(R^2\\). If we also know the variance of the predicted values on the non-linear scale \\(y\\), we have all the information to solve for \\(\\sigma_{\\hat{y}}\\), whose square-root is the standard deviation of \\(y\\) that we can use in the calculation of the standardized coefficient. This method, called the observation-empirical approach, does not require the acknowledgement of any latent variables or theoretical error variances, but does require an acceptance of this is a valid measurement of \\(R^2\\) (which some consider it not, as GLM estimation is based on deviance, not variance, and thus this statistic is not equivalent). It also does not provide a measure of the range of \\(y\\) although we can assume, againbased on sampling theory, that \\(6 * \\sigma_{y}\\) encompasses the full range of \\(y\\). Lets revisit our earlier GLM example and construct standardized coefficients: # get beta from model beta &lt;- summary(glm_model)$coefficients[2, 1] preds &lt;- predict(glm_model, type = &quot;link&quot;) # linear predictions # latent theoretic sd.ystar &lt;- sqrt(var(preds) + (pi^2)/3) # for default logit-link beta_lt &lt;- beta * sd(x)/sd.ystar # observation empirical R2 &lt;- cor(y, predict(glm_model, type = &quot;response&quot;))^2 # non-linear predictions sd.yhat &lt;- sqrt(var(preds)/R2) beta_oe &lt;- beta * sd(x)/sd.yhat # obtain using `coefs` coefs(glm_model, standardize.type = &quot;latent.linear&quot;); beta_lt ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 y x -2.0975 0.9664 18 -2.1703 0.03 -0.8122 * ## [1] -0.8121808 coefs(glm_model, standardize.type = &quot;Menard.OE&quot;); beta_oe ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 y x -2.0975 0.9664 18 -2.1703 0.03 -0.6566 * ## [1] -0.6565602 We see that both approaches produce valid coefficients and they are the same as those returned by the coefs function in piecewiseSEM (with the appropriate argument). Youll note that the observation-empirical approach yields a smaller coefficient than the latent-theoretic. This is because the former approach is influenced by the fact that it is based on the relationship between a linear approximation (predictions) of a non-linear variable (raw values), introducing a loss of information. The latent theoretic approach also suffers from a loss of information from use of a distribution-specific but theoretically-derived error variance for binomial distrbutions, which may or may not approach the true error variance (which is unknowable). Either way, both kinds of standardization are not without their drawbacks, but both provide potentially useful information in being able to compare linear and now linearized standardized coefficients for logistic regression. 4.5 Scaling to Other Non-Normal Distributions In many ways, logistic regression is the most demanding case for standardization because we are actually modeling a latent (unmeasurable) property whose variance cant be known (but as explained above, can be estimated). For other distributions, we are modeling the actual values: this means that only the observation-empirical approach can be used, which simplifies the procedure greatly as we dont need to worry about any theoretical error variances. Lets work through an example using the Poisson distribution for count data. The default link function for Poisson models is the log-link, which for our purposes means that we are simply modeling the log of the response. Therefore, we will assume that a generalized linear model fit to a Poisson distribution is approximately the same as a general linear model of the log-transformed response fit to a normal distribution (see papers by Ives and others on this topic). This approximate equivalency will become clear in a moment. First, lets create some example Poisson-distributed data: set.seed(100) count_data &lt;- data.frame(y = rpois(100, 10)) count_data$x &lt;- count_data$y * runif(100, 0, 5) Now lets fit an LM with the log-transformed response and see what kind of coefficient we recover: lm_model &lt;- lm(log(y) ~ x, count_data) coefs(lm_model)$Std.Estimate ## [1] 0.5346 As you may recall from the second rule of path coefficients, the standardized coefficient from a simple linear regression is actually the bivariate correlation between the two: with(count_data, cor(x, log(y))) ## [1] 0.5345506 And we see in this example that \\(b_x = r = 0.54\\). Now lets re-fit this model to \\(y\\) using GLM to a Poisson distribution with the default log-link: glm_model2 &lt;- glm(y ~ x, family = poisson(link = &quot;log&quot;), count_data) coef(glm_model2)[2] ## x ## 0.01204693 Here, \\(\\beta_x = 0.012\\) which is a bit different from the linear model. That is because the unstandardized coefficient is reported on the link scale. Lets repeat our observation-empirical procedure, first by getting the \\(R^2\\) which is the squared correlation between the raw vs. fitted values, then by getting the variance of the raw observations: R2 &lt;- cor(count_data$y, predict(glm_model2, type = &quot;response&quot;))^2 # non-linear predictions sd.yhat &lt;- sqrt(var(predict(glm_model2, type = &quot;link&quot;))/R2) coef(glm_model2)[2] * sd(count_data$x)/sd.yhat ## x ## 0.5695438 This value of \\(b_x = 0.57\\) from the GLM is remarkably close to the standardized coefficient obtained from the linear model, which was \\(b_x = 0.54\\) and also the correlation between \\(x\\) and \\(log(y)\\). The differences arise from the fact that for the linear model we have considered the error on \\(log(y)\\) whereas in the GLM, we have only considered the systematic but not the random component. They are generally small enough to be negligible if the data are truly Poisson-distributed. Therefore, we feel comfortable enough reporting the observation-empirical values from the GLM, noting again that they have a slight downward bias due to not incorporating the random component. Its important to note that by virtue of considering only the the total variance of the fitted values produced by the model, we can extend these methods to hierarchical, mixed, and other models where variance is partitioned or modeled. For distributions other than Poisson and negative binomial, the procedure becomes trickier. For example, here we assume the variance of the response equals the mean. However, other classessuch as the quasi-distributionsestimate an additional parameter \\(\\phi\\) to explain how the variance changes with the mean. As we are interested in quantifying this variance, it is not yet clear how to derive meaningful approximations of \\(sd_y\\) from such distributions. However, we hope to make significant progress on this front in the coming months, so this functionality may be incorporated into the piecewiseSEM package soon. 4.6 References Grace, J. B., Johnson, D. J., Lefcheck, J. S., &amp; Byrnes, J. E. (2018). Quantifying relative importance: computing standardized effects in models with binary outcomes. Ecosphere, 9(6), e02283. Scott Long, J. (1997). Regression models for categorical and limited dependent variables. Advanced quantitative techniques in the social sciences, 7. "],["categorical-variables.html", "5 Categorical Variables 5.1 Introduction to Exogenous Categorical Variables 5.2 Exogenous Categorical Variables as Marginal Means 5.3 Exogenous Categorical Variables as Marginal Means: A Worked Example 5.4 Endogenous Categorical Variables 5.5 References", " 5 Categorical Variables While SEM was initially derived to consider only continuous variables (and indeed most applications still do), its often the caseespecially in ecologythat the observed variables are discrete. For example: binary (yes/no, failure/success, etc.), nominal (site 1, site 2), or ordinal levels (small &lt; medium &lt; large). Newer advances in modeling allow for the incorporation, either directly or in a more roundabout fashion, of categorical variables into SEM. There are two way that categorical variables could be included: as exogenous (predictors) or endogenous (responses). We will deal with the simpler case of exogenous categorical variables first, as they pose not so much of a computational issue but a conceptual one. 5.1 Introduction to Exogenous Categorical Variables Recall that a linear regression predicting y has the following standard form: \\[y = \\alpha + \\beta_{1}*x_{1} + \\epsilon\\] where \\(\\alpha\\) is the intercept, \\(\\beta_{1}\\) is the slope of the effect of \\(x\\) on y, and \\(\\epsilon\\) is the residual error. When \\(x\\) is continuous, the intercept \\(\\alpha\\) is intepreted as the value of y when \\(x\\) = 0. All good. For categorical factors, the intercept \\(\\alpha\\) has a different interpretation. Consider a value of \\(x\\) with \\(k\\) levels. Since the levels of \\(x\\) are discrete and presumably can never assume a value of 0, \\(\\alpha\\) is instead the mean value of y at the reference level of \\(x\\). (In R, the reference level is the first level alphabetically, although this can be reset manually.) The regression coefficients \\(\\beta_{k}\\) are therefore the effect of each other level relative to the reference level. So for \\(k\\) levels, there are \\(k - 1\\) coefficients estimated with the additional \\(\\alpha\\) term reflecting the \\(k\\)th level. Another way to think about this phenomenon is using so-called dummy variables. Imagine each level was broken into a separate variable with a value of 0 or 1: a two-level factor with levels a and b would then become two factors a and b each with the levels 0 or 1. (In R, this would mean transposing rows as columns.) Now imagine setting all the values of these dummy variables to 0 to estimate the intercept: this would imply the total absence of the factor, which is not a state. Another way of thinking about this is that the dummy variables are linearly dependent: if a = 1 then by definition b = 0 as the response variable cannot occupy the two states simultaneously. Hence the need to set one level as the reference, so that the effect of a can be interpreted relative to the absence of b, and also why you dont recover as many coefficients as there are levels. I once heard it state that one level had to fall on the sword so that we can estimate the other levels. This behavior presents a challenge for parameterizing path diagrams: there is not a single coefficient for the path from \\(x\\) -&gt; \\(y\\), nor are there enough coefficients to populate a separate arrow for each level of \\(x\\) (because one level must serve as the reference). There are a few potential solutions: for binary variables, set the values as 0 or 1 and model as numeric, which would yield a single coefficient representing the expected change in \\(y\\) as \\(x\\) changes from state 0 to the other state 1. for ordinal variables, set the values depending on the order of the factor, e.g., small = 1 &lt; medium = 2 &lt; large = 3, and then model as numeric, which would also yield a single coefficient represented the expected change in \\(x\\) as you climb the ordinal ladder from smallest, to medium, and so on. create dummy variables for each level: this is procedurally the same as above (splitting levels into \\(k\\) - 1 separate variables that have a state of or/1). The key here is not to create \\(k\\) variables, to avoid the issue raised above about dependence among levels. This is the default behavior of lavaan. This approach becomes prohibitive with large number of categories or levels and can greatly increase model complexity. Moreover, each level is treated as an independent variable in the tests of directed separation, and thus will inflate the degrees of freedom in a piecewise application. for suspected interactions with categorical variables, a multigroup analysis is required. In this case, the same model is fit for each level of the factor, with potentially different coefficients (see the following chapter on Multigroup Modeling). test for the effect of the categorical variable using ANOVA, but do not report a coefficient. This approach would indicate whether a factor is important (i.e., whether the levels significantly differ with respect to the response), but omits important information about which levels and the direction and magnitude of change. For example, does a significant treatment effect imply an increase or decrease in the response, and by how much? For this reason, such an approach is valid but not ideal. A alternate approach draws on this final point and involves testing and reporting the model-estimated or marginal means. 5.2 Exogenous Categorical Variables as Marginal Means All models can be used for prediction. In multiple regression, the predicted values of one variable are often computed while holding the values of other variables at their mean. Marginal means are the averages of these predictions. In other words, they are the expected average value of one predictor given the other co-variables in the model. For categorical variables, marginal means are particularly useful because they provide an estimated mean for each level of each factor. Consider a simple example with a single response and two groups a and b: set.seed(111) dat &lt;- data.frame(y = runif(100), group = letters[1:2]) model &lt;- lm(y ~ group, dat) summary(model) ## ## Call: ## lm(formula = y ~ group, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.48473 -0.21466 -0.01238 0.19715 0.54995 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.44677 0.03871 11.541 &lt;2e-16 *** ## groupb 0.08551 0.05475 1.562 0.122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2737 on 98 degrees of freedom ## Multiple R-squared: 0.02429, Adjusted R-squared: 0.01433 ## F-statistic: 2.44 on 1 and 98 DF, p-value: 0.1215 Note that the summary output gives a simple coefficient, which is the effect of group b on y in the absence of group a. As we established above, the intercept is simply the average of y in group a: summary(model)$coefficients[1, 1] ## [1] 0.4467679 mean(subset(dat, group == &quot;a&quot;)$y) ## [1] 0.4467679 The marginal means are the expected average value of y in group a AND group b. predict(model, data.frame(group = &quot;a&quot;)) ## 1 ## 0.4467679 predict(model, data.frame(group = &quot;b&quot;)) ## 1 ## 0.53228 Because this is a simple linear regression, these values are simply the means of the two subsets of the data, because they are not controlling for any other covariates: mean(subset(dat, group == &quot;a&quot;)$y) ## [1] 0.4467679 mean(subset(dat, group == &quot;b&quot;)$y) ## [1] 0.53228 Lets see what happens we add a continuous covariate: dat$x &lt;- runif(100) model &lt;- update(model, . ~ . + x) Here, the marginal mean must be evaluated while holding the covariate \\(x\\) at its mean value: predict(model, data.frame(group = &quot;a&quot;, x = mean(dat$x))) ## 1 ## 0.4450597 mean(subset(dat, group == &quot;a&quot;)$y) ## [1] 0.4467679 Youll note that this value is now different than the mean of the subset of the data because, again, it controls for the effect of \\(x\\) on \\(y\\). This procedure gets increasingly complicated with both the number of factor levels and the number of covariates. The emmeans package provides an easy way to compute marginal means: library(emmeans) emmeans(model, specs = &quot;group&quot;) # where specs is the variable or list of variables whose means are to be estimated ## group emmean SE df lower.CL upper.CL ## a 0.445 0.0389 97 0.368 0.522 ## b 0.534 0.0389 97 0.457 0.611 ## ## Confidence level used: 0.95 Youll note that the output value for group a gives the same as using the predict function above, but also returns the marginal mean for group b while also controlling for \\(x\\): predict(model, data.frame(group = &quot;b&quot;, x = mean(dat$x))) ## 1 ## 0.5339882 and so is a handy wrapper for complex models. The emmeans function goes onto to provide lower and upper confidence intervals, which provides an additional level of information, namely whether each mean differs significantly from zero. Coupled with ANOVA test for differences among categories, the marginal means provide key information that is otherwise lacking, namely whether and how the response value changes based on the factor level. It is import The emmeans package provides additional functionality by conducting post-hoc tests of differences among the means of each factor level: emmeans(model, list(pairwise ~ group)) ## $`emmeans of group` ## group emmean SE df lower.CL upper.CL ## a 0.445 0.0389 97 0.368 0.522 ## b 0.534 0.0389 97 0.457 0.611 ## ## Confidence level used: 0.95 ## ## $`pairwise differences of group` ## 1 estimate SE df t.ratio p.value ## a - b -0.0889 0.0552 97 -1.611 0.1105 Youll note a second output which is the pairwise contrast between the means of groups a and b with an associated significance test. These pairwise Tukey tests provide the final level of information, which is whether the response in each level varies significantly from the other levels. To adapt this to SEM, the coefs function in piecewiseSEM adopts a two-tiered approach by first computing the significance of the categorical variable using ANOVA, and then reports the marginal means and post-hoc tests: library(piecewiseSEM) coefs(model) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 y x 0.0567 0.093 97 0.6094 0.5437 0.0613 ## 2 y group - - 1 2.5946 0.1105 - ## 3 y group = a 0.4451 0.0389 97 11.4300 0.0000 - *** ## 4 y group = b 0.534 0.0389 97 13.7139 0.0000 - *** In this output, we retrieve the normal output for the continuous \\(x\\) including a standardized effect size. The significance test from the ANOVA is reported in the row corresponding to the group effect, and below that are the marginal means for each level of the grouping factor. Note that there are no standardized estimates for either the ANOVA effect or the marginal means, because as we have established, these are not linear coefficients and therefore cannot be standardized as usual. This solution provides a measure of whether the path between the exogenous categorical variable and the response is significant as well as parameters for each level in the form of the model-estimated marginal means. 5.3 Exogenous Categorical Variables as Marginal Means: A Worked Example Lets consider an example from Bowen et al. (2017). In this study, the authors were interested in how different microbiomes of the salt marsh plant Phragmites australis drive ecosystem functioning, and ultimately the production of aboveground biomass. In this case, they considered three microbial communities: those from a native North American lineage, from Gulf Coast lineage, and an introduced lineage. There were additional genotypes within each community type, necessitating the application of random effects to account for intraspecific variation. We will fit a simplified version of their full path diagram, focusing only on aboveground biomass (although they test the effect on belowground biomass in their study as well). In this case, the variable Phragmites status corresponds to the three community types and cant be represented using a single coefficient. Thus, the marginal-means approach is ideal to elucidate the effect of each community type on both proximate and ultimate ecosystem properties while testing the overall significance of this path. Lets read in the data and construct the model: bowen &lt;- read.csv(&quot;https://raw.githubusercontent.com/jslefche/sem_book/master/data/bowen.csv&quot;) bowen &lt;- na.omit(bowen) library(nlme) bowen_sem &lt;- psem( lme(observed_otus ~ status, random = ~1|Genotype, data = bowen, method = &quot;ML&quot;), lme(RNA.DNA ~ status + observed_otus, random = ~1|Genotype, data = bowen, method = &quot;ML&quot;), lme(below.C ~ observed_otus + status, random = ~1|Genotype, data = bowen, method = &quot;ML&quot;), lme(abovebiomass_g ~ RNA.DNA + observed_otus + belowCN + status, random = ~1|Genotype, data = bowen, method = &quot;ML&quot;), data = bowen ) And lets retrieve the output: summary(bowen_sem, .progressBar = FALSE) ## Warning: Categorical or non-linear variables detected. Please refer to ## documentation for interpretation of Estimates! ## ## Structural Equation Model of bowen_sem ## ## Call: ## observed_otus ~ status ## RNA.DNA ~ status + observed_otus ## below.C ~ observed_otus + status ## abovebiomass_g ~ RNA.DNA + observed_otus + belowCN + status ## ## AIC ## 1110.030 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## observed_otus ~ belowCN + ... coef 57 1.4405 0.1552 ## RNA.DNA ~ belowCN + ... coef 56 1.3740 0.1749 ## below.C ~ belowCN + ... coef 56 2.7225 0.0086 ** ## below.C ~ RNA.DNA + ... coef 56 -0.1762 0.8607 ## abovebiomass_g ~ below.C + ... coef 54 -0.4540 0.6516 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 11.484 with P-value = 0.043 and on 5 degrees of freedom ## Fisher&#39;s C = 17.877 with P-value = 0.057 and on 10 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value ## observed_otus status - - 2 5.9589 0.0508 ## observed_otus status = native 2258.6564 105.8178 12 21.3448 0.0000 ## observed_otus status = introduced 2535.07 131.2216 14 19.3190 0.0000 ## observed_otus status = invasive 2541.4984 56.6887 12 44.8326 0.0000 ## RNA.DNA observed_otus 0 0 57 2.1583 0.0351 ## RNA.DNA status - - 2 9.3480 0.0093 ## RNA.DNA status = invasive 0.7112 0.0118 12 60.4511 0.0000 ## RNA.DNA status = introduced 0.7305 0.0264 14 27.6880 0.0000 ## RNA.DNA status = native 0.7844 0.0216 12 36.2988 0.0000 ## below.C observed_otus 9e-04 3e-04 57 2.6546 0.0103 ## below.C status - - 2 17.1995 0.0002 ## below.C status = introduced 42.6366 0.3673 14 116.0774 0.0000 ## below.C status = invasive 43.4004 0.1594 12 272.2299 0.0000 ## below.C status = native 44.4975 0.3056 12 145.6071 0.0000 ## abovebiomass_g RNA.DNA -1.8517 1.8893 55 -0.9801 0.3313 ## abovebiomass_g observed_otus -2e-04 2e-04 55 -0.7521 0.4552 ## abovebiomass_g belowCN 0.005 0.0041 55 1.2026 0.2343 ## abovebiomass_g status - - 2 12.9519 0.0015 ## abovebiomass_g status = native 1.7489 0.2206 12 7.9272 0.0000 ## abovebiomass_g status = invasive 1.8807 0.1041 12 18.0629 0.0000 ## abovebiomass_g status = introduced 2.7024 0.232 14 11.6457 0.0000 ## Std.Estimate ## - ## - *** ## - *** ## - *** ## 0.135 * ## - ** ## - *** ## - *** ## - *** ## 0.2913 * ## - *** ## - *** ## - *** ## - *** ## -0.1428 ## -0.0905 ## 0.1356 ## - ** ## - *** ## - *** ## - *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method Marginal Conditional ## observed_otus none 0.10 0.19 ## RNA.DNA none 0.31 0.81 ## below.C none 0.26 0.32 ## abovebiomass_g none 0.22 0.28 In this case, it appears that the model fits the data well enough based on the Fishers C statistic (P = 0.057), which is what the original authors used. Note that the likelihood-based \\(\\chi^2\\) statistic actually implies poor fit (P = 0.043). In this case, examining the d-sep tests suggests the inclusion of the path from \\(belowCN\\) to \\(below.C\\) might improve the fit. However, we are applying a tool that was not available to the original authors, so we will proceed as they did and assume adequate fit. The linkage between microbial community type (\\(status\\)) and richness (\\(observed_otus\\)) is non-significant, but the other paths are significant. Examination of the marginal means indicates microbial activity (\\(RNA.DNA\\)) and belowground carbon (\\(below.C\\)) are generally highest in Phragmites with native microbial communities based on the post-hoc tests. However, none of these properties appear to influence the ultimate production of biomass (\\(abovebiomass_g\\)). Rather, that property appears to be entirely controlled by the plant microbiome type (\\(status\\)): those with the introduced microbial community have significantly higher aboveground biomass based on their marginal mean after controlling for microbial activity and soil nutrients. Thus, despite a multi-level categorical predictor (\\(status\\)), the two-step procedure of ANOVA and calculation of marginal means allows for a more mechanistic understanding of the drivers of plant biomass in this species. 5.4 Endogenous Categorical Variables Endogenous categorical variables are far trickier, and at the moment, are not implemented in piecewiseSEM. In the case of endogenous categorical variables in a piecewise framework, there are really only two solutions: for binary variables, set the values as 0 or 1 and model as numeric, which would yield a single coefficient. for ordinal variables, set the values depending on the order of the factor, e.g., small = 1 &lt; medium = 2 &lt; large = 3, and then model as numeric, which would yield a single coefficient. Nominal variables (i.e., levels are not ordered) could be modeled using multinomial regression, although this method would have to be executed by hand. An alternative is to use the factor levels to construct a composite variable, the subject of a later chapter. lavaan provides a robust alternative in the form of confirmatory factor analysis (see http://lavaan.ugent.be/tutorial/cat.html). In piecewiseSEM, composites must be constructed by hand, although this procedure is not hugely prohibitive. 5.5 References Bowen, J. L., Kearns, P. J., Byrnes, J. E., Wigginton, S., Allen, W. J., Greenwood, M.,  &amp; Meyerson, L. A. (2017). Lineage overwhelms environmental conditions in determining rhizosphere bacterial community structure in a cosmopolitan invasive plant. Nature communications, 8(1), 433. "],["multigroup-analysis.html", "6 Multigroup Analysis 6.1 Introduction to Multigroup Analysis 6.2 Multigroup Analysis using Global Estimation 6.3 Multigroup Analysis Using Local Estimation 6.4 Grace &amp; Jutila (1999): A Worked Example 6.5 References", " 6 Multigroup Analysis 6.1 Introduction to Multigroup Analysis Often in ecology we wish to compare the results from two or more groups. These groups could reflect experimental treatments, different sites, different sexes, or any number of types of organization. The ultimate goal of such an analysis is to ask whether the relationships among predictor and response variables vary by group. Historically, such a goal would be captured through the application of a statistical interaction. For example, does the effect of pesticide on invertebrate biomass change as function of where the pesticide is applied? This model might look something like this: \\[biomass = pesticide \\times location\\] A significant interaction between \\(pesticide \\times location\\) would indicate that the effect of pesticide application on invertebrate biomass indeed varies by location. It would of course then be up to the author to use their knowledge of the system to speculate why this is. In the event that the interaction is not statistically significant, the author would conclude that the effect of pesticide is invariant to location. The author could instead generalize the effects of pesticide, such that its expected to have the same magnitude of effect regardless of where its applied. A multigroup model is essentially the same principle, but instead of focusing on a single response, the interaction is applied across the entire structural equation model. In other words, it asks if not just one but all coefficients are the same or different across groups. In a sense, it can be thought of as a model-wide interaction, and, in fact, this is how we will treat it later using a piecewise approach. Of course, you might ask: why not simply fit the same model structure to different subsets of the data? Unfortunately, this would not allow you to identify which paths change based on the group and which do not, which could be key insight. Rather, one would have to compare the magnitude and standard errors of each pair of coefficients manually, rather than through a formal statistical procedure. The application of multigroup models differs between a global estimation (i.e., variance-covariance based SEM) and local estimation (i.e., piecewise SEM), but they adhere to the same idea of identifying which paths have the same effect across groups and which paths vary depending on the group. In this chapter, we will work through both approaches and then compare/contrast the output. 6.2 Multigroup Analysis using Global Estimation Multigroup modeling using global estimation begins with the estimation of two models: one in which all parameters are allowed to differ between groups, and one in which all parameters are fixed to those obtained from analysis of the pooled data across groups. We call the first model the free model since all parameters are free to vary and the second the constrained model since each path, regardless of its group, is constrained to a single value determined by the entire dataset. If the two models are not significantly different, and the latter fits the data well, then one can assume there is no variation in the path coefficients by group and multigroup approach is not necessary. In this case, the output from the constrained model would be reported. If they are, then the exercise shifts towards understanding which paths are the same and which are different. This is achieved by sequentially constraining the coefficients of each path and re-fitting the model. Lets illustrate this procedure using a random example using three variables\\(x\\), \\(y\\), and \\(z\\)in two groups: a and b. set.seed(111) dat &lt;- data.frame(x = runif(100), group = rep(letters[1:2], each = 50)) dat$y &lt;- dat$x + runif(100) dat$z &lt;- dat$y + runif(100) In this example, we suppose a simple mediation model: \\(x -&gt; y -&gt; z\\), and that all three variables are correlated to some degree so that this model makes sense. We can use lavaan to fit the free model. The key is allowing the coefficients to vary by specifying the group = argument: multigroup.model &lt;- &#39; y ~ x z ~ y &#39; library(lavaan) multigroup1 &lt;- sem(multigroup.model, dat, group = &quot;group&quot;) We can then obtain the summary of the multigroup analysis: summary(multigroup1) ## lavaan 0.6-9 ended normally after 38 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations per group: ## a 50 ## b 50 ## ## Model Test User Model: ## ## Test statistic 0.092 ## Degrees of freedom 2 ## P-value (Chi-square) 0.955 ## Test statistic for each group: ## a 0.049 ## b 0.043 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [a]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.771 0.163 4.734 0.000 ## z ~ ## y 1.080 0.126 8.577 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.684 0.088 7.745 0.000 ## .z 0.463 0.140 3.313 0.001 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.080 0.016 5.000 0.000 ## .z 0.092 0.018 5.000 0.000 ## ## ## Group 2 [b]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 1.240 0.135 9.182 0.000 ## z ~ ## y 0.897 0.086 10.465 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.349 0.078 4.460 0.000 ## .z 0.612 0.092 6.654 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.082 0.016 5.000 0.000 ## .z 0.081 0.016 5.000 0.000 Note that, unlike the typical lavaan output, the printout is now organized by group, with separate coefficients for each path in each group. Because this model is allowed to vary, the coefficient for the \\(x -&gt; y\\) path in group a is different, for example, from that reported for group b. Next, we fit the constrained model by specifying the additional argument group.equal = c(\"intercepts\", \"regressions\"). This argument fixes both the intercepts and path coefficients in each group to be the same. multigroup1.constrained &lt;- sem(multigroup.model, dat, group = &quot;group&quot;, group.equal = c(&quot;intercepts&quot;, &quot;regressions&quot;)) summary(multigroup1.constrained) ## lavaan 0.6-9 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## Number of equality constraints 4 ## ## Number of observations per group: ## a 50 ## b 50 ## ## Model Test User Model: ## ## Test statistic 9.951 ## Degrees of freedom 6 ## P-value (Chi-square) 0.127 ## Test statistic for each group: ## a 5.541 ## b 4.410 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [a]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x (.p1.) 1.046 0.108 9.678 0.000 ## z ~ ## y (.p2.) 0.960 0.072 13.413 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y (.p6.) 0.499 0.061 8.219 0.000 ## .z (.p7.) 0.570 0.078 7.283 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.087 0.017 5.000 0.000 ## .z 0.094 0.019 5.000 0.000 ## ## ## Group 2 [b]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x (.p1.) 1.046 0.108 9.678 0.000 ## z ~ ## y (.p2.) 0.960 0.072 13.413 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y (.p6.) 0.499 0.061 8.219 0.000 ## .z (.p7.) 0.570 0.078 7.283 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.089 0.018 5.000 0.000 ## .z 0.083 0.017 5.000 0.000 This output is slightly different from the first: the coefficients are reported by group, but they are now identical between groups (e.g., \\(\\gamma_x\\) in group a = \\(\\gamma_x\\) in group b). The constrained paths are indicated by a parenthetical next to the path (e.g., (.p1.) for path 1). Both the constrained and free models fit the data well based on the \\(\\chi^2\\) statistic, and we can formally compare the two using a Chi-squared difference test: anova(multigroup1, multigroup1.constrained) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## multigroup1 2 95.392 126.65 0.0921 ## multigroup1.constrained 6 97.251 118.09 9.9508 9.8588 4 0.04288 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significant P-value implies that the free and constrained models are significantly different. In other words, some paths vary while others may not. If the models were not significantly different, then one would conclude that the constrained model is equivalent to the free model, or that the coefficients would not vary by group, and it would be fair to analyze the pooled data in a single global model. However, this is the not the case for this example, and we can now undergo the process of introducing and releasing constraints to try and identify which path varies between groups. In this simplified example, we have two choices: \\(x -&gt; y\\), and \\(y -&gt; z\\). Lets focus on \\(x -&gt; y\\) first. We can introduce a single constraint by modifying the model formula and re-fitting the model: multigroup.model2 &lt;- &#39; y ~ c(&quot;b1&quot;, &quot;b1&quot;) * x z ~ y &#39; multigroup2 &lt;- sem(multigroup.model2, dat, group = &quot;group&quot;) The string c(\"b1\", \"b1\") gives the path the name b1 and ensures the coefficient is equal between the two groups (hence the two entries). If we use a Chi-squared difference test as before: anova(multigroup1, multigroup2) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## multigroup1 2 95.392 126.65 0.0921 ## multigroup2 3 98.188 126.84 4.8881 4.796 1 0.02853 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We find that the models are still significantly different, implying that the path between \\(x -&gt; y\\) should not be constrained and instead that it should be left to vary among groups. We can repeat this exercise with the second path, \\(y -&gt; z\\): multigroup.model3 &lt;- &#39; y ~ x z ~ c(&quot;b2&quot;, &quot;b2&quot;) * y &#39; multigroup3 &lt;- sem(multigroup.model3, dat, group = &quot;group&quot;) anova(multigroup1, multigroup3) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## multigroup1 2 95.392 126.65 0.0921 ## multigroup3 3 94.823 123.48 1.5230 1.4309 1 0.2316 In this case, there is not a significant difference between the two models (P = 0.23), implying that the is no difference in the fit of the constrained model and the unconstrained model and that this constraint is valid. If we were to select across these three alternatives, we would select the third model in which \\(x -&gt; y\\) is allowed to vary and \\(y -&gt; z\\) is constrained among groups. Its key to note that this model also fits the data well based on the \\(\\chi^2\\) statistic; if not, then like all poor-fitting path models (multigroup or otherwise) it would be unwise to present and draw conclusions from it. This exercise of relaxing and imposing constraints is potentially very exploratory and could become exhaustive with more complicated models (i.e., one with lots of paths to potentially constrain/relax). Users should refrain from constraining and relaxing all paths and then choosing the most parsimonious model. Instead, choosing which paths to constrain should be motivated by the question: for example, we might expect some effects to be universal (e.g., temperature on metabolic rate) but not others (e.g., the effect of pesticide may vary depending on the history of application at various sites). Critically, the degrees of freedom for the model do not change based on the number of groups because coefficients are estimated from variance-covariance matrices with the same dimensions. However, sample size must be sufficiently large to estimate all the parameters within each group with minimal bias. While this is true for all structural equation models, it is especially true where parameters are estimated from covariances derived from different subsets (groups) that do not have the same replication. Standardized coefficients also present a challenge. Because variances are likely to be unequal among groups (unless they are drawn from the same population), the standardized coefficient must be computed on a per group basis, even if the unstandardized coefficient is constrained to the global value. Both packages for SEM will do this automatically, so you may notice that the standardized solutions mayand more than often willvary even among constrained paths. 6.3 Multigroup Analysis Using Local Estimation The goal of multigroup analysis using local estimation is identical to that of global estimation: to identify whether a single global model is sufficient to describe the data, or whether some or all paths vary by some grouping variable. The difference lies in execution: while lavaan is a back-and-forth manual process of relaxing and constraining paths, piecewiseSEM tests constraints and automatically selects the best output for your data. The upside is that the arduous and somewhat cumbersome process of specifying constraints is taken care of; the downside is that manually constraining particular paths is not possible at this time, although forthcoming work by Shipley and Douma will make this possible. The first step in the local estimation process is to implement a model-wide interaction. In other words, every term in the model interacts with the grouping variable. If the interaction is significant, then the path is free to vary by group; if not, then the path takes on the estimate from the global dataset. In this way, the piecewise multigroup procedure breaks down into a series of classic interaction terms: it is literally and figuratively the model-wide interaction we discussed at the beginning of this chapter Consider our previous example fitted using lavaan. In a piecewise approach, we would first model the interaction between \\(x \\times group\\) to see whether the effect of \\(x\\) on \\(y\\) should vary by group: anova(lm(y ~ x * group, dat)) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 8.2740 8.2740 97.9518 2.475e-16 *** ## group 1 0.2772 0.2772 3.2811 0.07321 . ## x:group 1 0.3974 0.3974 4.7051 0.03254 * ## Residuals 96 8.1091 0.0845 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this case, the effect of \\(x\\) on \\(y\\) depends on \\(group\\) (a significant interaction term). We would then estimate the effect of \\(x\\) and \\(y\\) for each subset of the data, and report the coefficients separately. = Next, we evaluate the effect of \\(y\\) on \\(z\\) by group: anova(lm(z ~ y * group, dat)) ## Analysis of Variance Table ## ## Response: z ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## y 1 15.8899 15.8899 176.3764 &lt;2e-16 *** ## group 1 0.0366 0.0366 0.4066 0.5252 ## y:group 1 0.1271 0.1271 1.4107 0.2379 ## Residuals 96 8.6487 0.0901 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The second interaction between \\(y \\times group\\) in predicting \\(z\\) is non-significant, indicating that the effect of \\(y\\) on \\(z\\) does not depend on \\(group\\). We would then estimate the effect of \\(y\\) on \\(z\\) using the entire dataset and report that single constrained coefficient across all groups. The implementation of this approach in piecewiseSEM is very straightforward: first, build the model using psem, then use the function multigroup to perform the multigroup analysis. library(piecewiseSEM) pmodel &lt;- psem( lm(y ~ x, dat), lm(z ~ y, dat) ) The multigroup function has an argument group = which, as in lavaan, accepts the column name of the grouping factor: (pmultigroup &lt;- multigroup(pmodel, group = &quot;group&quot;)) ## ## Structural Equation Model of pmodel ## ## Groups = group [ a, b ] ## ## --- ## ## Global goodness-of-fit: ## ## Fisher&#39;s C = 0.301 with P-value = 0.86 and on 2 degrees of freedom ## ## --- ## ## Model-wide Interactions: ## ## Response Predictor Test.Stat DF P.Value ## y x:group 8.3 1 0.0325 * ## z y:group 15.5 1 0.2379 ## ## y -&gt; z constrained to the global model ## ## --- ## ## Group [a] coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## y x 0.7712 0.1662 48 4.6387 0 0.5563 *** ## z y 0.9652 0.0726 98 13.2931 0 0.6895 *** c ## ## Group [b] coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## y x 1.2404 0.1379 48 8.9963 0 0.7923 *** ## z y 0.9652 0.0726 98 13.2931 0 0.8914 *** c ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 c = constrained If we examine the output, we see the output table of model-wide interactions. Its important to note that the package uses car::Anova with type = \"II\" sums-of-squares to estimate the interactions by default, but other types (e.g., type III) are accepted using the test.type = argument (or type I using the base anova function). As above, only the path from \\(x\\) -&gt; \\(y\\) is significantly different among groups. In this case, the function explicitly reports that the path y -&gt; z constrained to the global model. Next, as in lavaan, are the coefficient tables for each group. Values that have been constrained are the same between the two models and indicated with a c at the end of the row, while the unconstrained path from \\(x -&gt; y\\) is different between groups a and b. Its important to note that the standardized coefficients do differ for each group even though the paths are constrained. Again, this is because the variance differs between groups. Thus the standardization: $$\\beta_{std} = \\beta*\\left( \\frac{sd_{x}}{sd_{y}} \\right)$$ must consider only the standard deviation of x and y from their respective groups, even though \\(\\beta\\) is derived from the entire dataset. Finally, near the top is the global goodness-of-fit test based on Fishers C. In this case, global constraints have been added as offset to the tests of directed separation. For comparisons sake, lets look at the output from the lavaan multigroup model and the piecewiseSEM one: standardizedSolution(multigroup3) ## lhs op rhs group label est.std se z pvalue ci.lower ci.upper ## 1 y ~ x 1 0.556 0.090 6.197 0.000 0.380 0.732 ## 2 z ~ y 1 b2 0.728 0.054 13.556 0.000 0.623 0.833 ## 3 y ~~ y 1 0.690 0.100 6.912 0.000 0.495 0.886 ## 4 z ~~ z 1 0.470 0.078 6.014 0.000 0.317 0.623 ## 5 x ~~ x 1 1.000 0.000 NA NA 1.000 1.000 ## 6 y ~1 1 2.012 0.406 4.953 0.000 1.216 2.808 ## 7 z ~1 1 1.336 0.259 5.158 0.000 0.828 1.844 ## 8 x ~1 1 1.971 0.000 NA NA 1.971 1.971 ## 9 y ~ x 2 0.792 0.044 18.167 0.000 0.707 0.878 ## 10 z ~ y 2 b2 0.843 0.036 23.508 0.000 0.773 0.913 ## 11 y ~~ y 2 0.372 0.069 5.387 0.000 0.237 0.508 ## 12 z ~~ z 2 0.289 0.060 4.782 0.000 0.171 0.408 ## 13 x ~~ x 2 1.000 0.000 NA NA 1.000 1.000 ## 14 y ~1 2 0.742 0.213 3.480 0.001 0.324 1.160 ## 15 z ~1 2 1.045 0.210 4.976 0.000 0.633 1.456 ## 16 x ~1 2 1.650 0.000 NA NA 1.650 1.650 pmultigroup$group.coefs ## $a ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 y x 0.7712 0.1662 48 4.6387 0 0.5563 *** ## 2 z y 0.9652 0.0726 98 13.2931 0 0.6895 *** ## ## 1 ## 2 c ## ## $b ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 y x 1.2404 0.1379 48 8.9963 0 0.7923 *** ## 2 z y 0.9652 0.0726 98 13.2931 0 0.8914 *** ## ## 1 ## 2 c Youll note that the outputs are roughly equivalent (owing to slight differences in the estimation procedures for each package). Critically, the coefficient for the path from \\(z -&gt; y\\) is the same in both groups. 6.4 Grace &amp; Jutila (1999): A Worked Example Lets now turn to a real example from Grace &amp; Jutila (1999). While the original paper fit a far more complicated model than we will, the following simplified model demonstrates the approach well. In their study, the authors were interested in the controls of on plant species density in Finnish meadows. In this worked example, we will consider only elevation and total biomass in their effects on density, plus an effect of elevation on biomass: Moreover, they repeated their observations in two treatments: grazed and ungrazed meadows. Grazing will serve as the grouping variable for our multigroup analysis. The data are included in piecewiseSEM so lets load it: data(meadows) First, lets construct the free model in lavaan: jutila_model &lt;- &#39; rich ~ elev + mass mass ~ elev &#39; jutila_lavaan_free &lt;- sem(jutila_model, meadows, group = &quot;grazed&quot;) summary(jutila_lavaan_free) ## lavaan 0.6-9 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations per group: ## 1 165 ## 0 189 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## Test statistic for each group: ## 1 0.000 ## 0 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [1]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## elev 0.073 0.010 7.232 0.000 ## mass -0.001 0.002 -0.424 0.672 ## mass ~ ## elev -1.203 0.470 -2.559 0.010 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 7.169 0.708 10.126 0.000 ## .mass 260.855 26.764 9.746 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 12.459 1.372 9.083 0.000 ## .mass 28057.590 3089.039 9.083 0.000 ## ## ## Group 2 [0]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## elev 0.088 0.011 7.988 0.000 ## mass -0.007 0.001 -5.465 0.000 ## mass ~ ## elev -3.274 0.554 -5.908 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 11.349 0.750 15.139 0.000 ## .mass 451.732 24.949 18.107 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 14.384 1.480 9.721 0.000 ## .mass 43567.994 4481.792 9.721 0.000 In this example, the model goodness-of-fit fit cant be determined because the model is saturated (df = 0). This is key moving forward because constraining paths will free up degrees of freedom with which to obtain a test statistic. Also note the warning message above variances being a factor 1000 times larger than others. This can be problematic for the estimation of the variance-covariance matrix. The simple solution is the use the function scale or add/subtract a constant to reduce these values before fitting the model. Lets begin by constraining all paths: jutila_lavaan_constrained &lt;- sem(jutila_model, meadows, group = &quot;grazed&quot;, group.equal = c(&quot;intercepts&quot;, &quot;regressions&quot;)) anova(jutila_lavaan_free, jutila_lavaan_constrained) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## jutila_lavaan_free 0 6666.2 6720.3 0.000 ## jutila_lavaan_constrained 5 6754.4 6789.2 98.261 98.261 5 &lt; 2.2e-16 ## ## jutila_lavaan_free ## jutila_lavaan_constrained *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model is significantly different from the unconstrained model we fit previously (P &lt; 0.001), implying that some paths could be constrained. Note that, by constraining all the coefficients, we now have 5 degrees of freedom to evaluate model fit. (However, if we were to examine it, we would find that it is a poor fit, implying that some path coefficients must vary among groups.) The next step is to sequentially relax and constrain paths: jutila_model2 &lt;- &#39; rich ~ elev + mass mass ~ c(&quot;b1&quot;, &quot;b1&quot;) * elev &#39; jutila_lavaan2 &lt;- sem(jutila_model2, meadows, group = &quot;grazed&quot;) anova(jutila_lavaan_free, jutila_lavaan2) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## jutila_lavaan_free 0 6666.2 6720.3 0.0000 ## jutila_lavaan2 1 6672.2 6722.5 8.0301 8.0301 1 0.004601 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model is still a poor fit, and it is significantly different from the free model (P = 0.005). In this case, we would conclude that the \\(elev -&gt; mass\\) path should not be constrained. Lets repeat for the next two paths: # elev -&gt; rich jutila_model3 &lt;- &#39; rich ~ c(&quot;b2&quot;, &quot;b2&quot;) * elev + mass mass ~ elev &#39; jutila_lavaan3 &lt;- sem(jutila_model3, meadows, group = &quot;grazed&quot;) anova(jutila_lavaan_free, jutila_lavaan3) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## jutila_lavaan_free 0 6666.2 6720.3 0.0000 ## jutila_lavaan3 1 6665.1 6715.4 0.9477 0.94767 1 0.3303 # mass -&gt; rich jutila_model4 &lt;- &#39; rich ~ elev + c(&quot;b3&quot;, &quot;b3&quot;) * mass mass ~ elev &#39; jutila_lavaan4 &lt;- sem(jutila_model4, meadows, group = &quot;grazed&quot;) anova(jutila_lavaan_free, jutila_lavaan4) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## jutila_lavaan_free 0 6666.2 6720.3 0.0000 ## jutila_lavaan4 1 6673.6 6723.9 9.4642 9.4642 1 0.002095 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Of these two paths, it seems the first: \\(elev -&gt; rich\\), is not significantly different from the free model, implying that this path could be constrained. Oppositely, the significant difference between the free model and one in which the \\(mass -&gt; rich\\) path is constrained suggests that constraining this path is not supported. Lets check the fit of the model with the one constrait on \\(elev -&gt; rich\\): summary(jutila_lavaan3) ## lavaan 0.6-9 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 1 ## ## Number of observations per group: ## 1 165 ## 0 189 ## ## Model Test User Model: ## ## Test statistic 0.948 ## Degrees of freedom 1 ## P-value (Chi-square) 0.330 ## Test statistic for each group: ## 1 0.435 ## 0 0.513 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [1]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## elev (b2) 0.080 0.007 10.717 0.000 ## mass -0.000 0.002 -0.297 0.767 ## mass ~ ## elev -1.203 0.470 -2.559 0.010 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 6.795 0.596 11.408 0.000 ## .mass 260.855 26.764 9.746 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 12.492 1.375 9.083 0.000 ## .mass 28057.590 NA ## ## ## Group 2 [0]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## elev (b2) 0.080 0.007 10.717 0.000 ## mass -0.008 0.001 -5.999 0.000 ## mass ~ ## elev -3.274 0.554 -5.908 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 11.754 0.624 18.831 0.000 ## .mass 451.732 24.949 18.107 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 14.423 1.484 9.721 0.000 ## .mass 43567.993 Now the model fits the data well (P = 0.330), and we have, through an iterative procedure of imposing and relaxing constraints, determined which paths differ among groups (\\(elev -&gt; mass\\), \\(mass -&gt; rich\\)) and which do not (\\(elev -&gt; rich\\)). Now lets confirm this by fitting the model in piecewiseSEM: jutila_psem &lt;- psem( lm(rich ~ elev + mass, meadows), lm(mass ~ elev, meadows) ) multigroup(jutila_psem, group = &quot;grazed&quot;) ## ## Structural Equation Model of jutila_psem ## ## Groups = grazed [ 1, 0 ] ## ## --- ## ## Global goodness-of-fit: ## ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## ## Model-wide Interactions: ## ## Response Predictor Test.Stat DF P.Value ## rich elev:grazed 1556.7 1 0.3358 ## rich grazed:mass 1556.7 1 0.0026 ** ## mass elev:grazed 1416938.0 1 0.0055 ** ## ## elev -&gt; rich constrained to the global model ## ## --- ## ## Group [1] coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## rich elev 0.0731 0.0081 351 8.9882 0.0000 0.4967 *** ## rich mass -0.0007 0.0017 162 -0.4198 0.6752 -0.0291 ## mass elev -1.2028 0.4728 163 -2.5438 0.0119 -0.1954 * ## ## c ## ## ## ## Group [0] coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## rich elev 0.0731 0.0081 351 8.9882 0 0.3933 *** ## rich mass -0.0072 0.0013 186 -5.4216 0 -0.3222 *** ## mass elev -3.2735 0.5571 187 -5.8764 0 -0.3948 *** ## ## c ## ## ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 c = constrained As in our analysis in lavaan, the multigroup function has identified the \\(elev -&gt; rich\\) path as the only one in which coefficients do not differ among groups (again, denoted by a c next to the output). Thus, in the output, that coefficient is the same between groups; otherwise, the coefficients vary depending on whether the meadows is grazed or ungrazed. Moreover, it seems some of the paths differ in their statistical significance: the \\(rich -&gt; mass\\) is not significant in the grazed meadows, but is significant in the ungrazed meadows. So not only do the coefficients differ, but the model structure as well. Youll note that the piecewiseSEM output does not return a goodness-of-fit test because the model is saturated (i.e., no missing paths). While constraints are incorporated in terms of offsets (i.e., fixing model coefficients), unlike global estimation, this procedure does not provide new information with which to test goodness-of-fit. This is a limitation of local estimation that extends beyond multigroup modeling to any piecewise model. To draw inference about the study system, we would say that two paths differ among groups and one path does not. We would then report the two path models parameterized using the coefficient output (with the \\(elev -&gt; rich\\) path having the same coefficient in both groups). We would conclude that richness is affected by elevation and biomass under ungrazed conditions, but not under grazed conditions, where only elevation directly influences richness. 6.5 References Grace, J. B., &amp; Jutila, H. (1999). The relationship between species density and community biomass in grazed and ungrazed coastal meadows. Oikos, 398-408. "],["latent-variable-modeling.html", "7 Latent Variable Modeling 7.1 Introduction to Latent Variable Modeling 7.2 Application of Latent Variables to Path Models 7.3 Latent Variables in lavaan 7.4 Multi-indicator Latent Variables 7.5 Confirmatory Factor Analysis 7.6 Travis &amp; Grace (2010): An Example 7.7 References", " 7 Latent Variable Modeling 7.1 Introduction to Latent Variable Modeling Latent variables are variables that are unobserved, but whose influence can be summarized through one or more indicator variables. They are useful for capturing complex or conceptual properties of a system that are difficult to quantify or measure directly. Early applications of latent variables, for example, focused on modeling the effects of general intelligence, which is an abstract concept that is impossible to actually measure, but that can be approximated using scores from different tests of cognitive performance (e.g., memory, verbal, spatial, etc.). Consider the following simple example of a latent variable \\(\\xi\\), in this case exogenous and informed only by a single predictor \\(x\\): Here, the latent variable is indicated by the circle and the single indicator variable \\(x\\) is indicated by the square box, as are all observed variables. Youll note a few curiosities compared to observed-variable models. First, the direction of causality is reversed from what you might expect: from the latent variables to the observed variable. This is because the indicator variable is an emergent manifestation of the underlying phenomenon represented by the latent variable. Second, there is an error \\(\\delta\\) associated with the indicator. This implies that the indicator is often an imperfect approximation of the latent construct. In other words, there are presumably other factors influencing the correlation between the observed and latent variable. The latent variable can be related to the indicator variable using the following equation: \\[x = \\lambda \\xi + \\delta_{x}\\] Here, the values of \\(x\\) are the result of the latent variable proportional to \\(\\lambda\\) (its effect on \\(x\\)) plus some error \\(\\delta_{x}\\). A simple example of a latent-indicator relationship would be body size (latent) and body mass (indicator). There are obviously many aspects to body size that may be difficult to quantify, such as shape, volume, relief, and so on. Body mass is a simple, measurable consequence of these unmeasured characteristics, and thus can be thought to latently indicate the concept of size. However, because we often cant perfectly measure body mass of every individual in the population in which we are interested, we must incorporate sampling error into our model of body size, indicated by the \\(\\delta_{x}\\). This example reinforces the point that latent variables are used to represent concepts. Body size is often invoked in lots of ecological hypotheses (e.g., metabolic theory, Bergmanns rule), but is almost always represented as some easily measurable quantity such as body mass rather than the complex, multidimensional construct that it is in reality. Latent variable modeling allows us to better approach that multidimensional construct by modeling a series of indicator variables that arise from the general concept of body size (e.g., mass, length, width, etc.). It therefore is a powerful tool that is better positioned to integrate theory and observation than relying on one or few surrogates. However, some care should be taken when constructing latent variables. Just because we call a latent variable something does not always mean it is that thing. For example, the latent variable body size as indicated by total abundance might appear legitimatehigh abundances may constrain body sizes under limited resourcesbut is abundance really an indicator of this phenomenon? Can we go on to evaluate ecological theory about metabolic scaling on the basis of abundances? Kind of a stretch. So care should be taken when selecting or naming latent variables and identifying appropriate indicators (known as the naming fallacy). In other words: be sure the latent variable reflects the actual properties captured by the indicator variables! The degree to which the indicators represent the phenomenon captured by the latent variable is termed validity and is a qualitative justification of the latent construct. In contrast, the reliability of the latent variable captures how well an indicator reflects the latent variable through a quantitative value. High reliability implies that the same values of the indicator would be obtained if they were continually resampled again and again. In other words, reliable indicators approach the true population mean that is the (theoretical) product of the latent variable: a perfect indicator would yield the same values every time so they would all have a correlation \\(r = 1\\). Of course, rarely do we sample an entire population so well or so exhaustively, and there will inevitably be some differences among our samples leading to deviations in \\(r\\) away from 1. From the actual observed correlation among repeated measures of \\(x\\), we can obtain a path coefficient from the latent variable to the indicator. Recall from the fifth rule of path coefficients that the coefficient on the path from the error variance \\(\\zeta\\) is the square-root of the unexplained variance. In this case, we want the opposite: we want the shared variance between the latent and indicator variable (a lot of shared variance is what makes a good indicator!). As in the case of the error path, the path coefficient from the latent variable to the indicator is often expressed in its standardized form: the square-root of the reliability. This value is also called the loading. From the reliability, we can also obtain the standardized error term \\(\\delta_{x}\\). This is the unshared variance, or 1 - the reliability. For the unstandardized form, one can apply the following equation: \\[\\delta_{x} = (1 - \\lambda_{x}^2) \\times VAR_{x}\\] As with other coefficients, standardization is applied simply because multiple indicators may be measured in vastly different units, and one may wish to fairly compare the loadings and errors. Lets construct a simple example. Say we sample the variable \\(x\\) repeatedly 5 times with \\(n = 10\\). This could be 5 sampling dates or 5 separate trials. set.seed(11) x &lt;- rnorm(10) x.list &lt;- lapply(1:5, function(i) x + runif(10, 0, 2)) x. &lt;- unlist(x.list) We can compute the average correlation among all trials. This is our measure of reliability: combos &lt;- combn(1:5, 2) cors &lt;- c() for(i in 1:ncol(combos)) cors &lt;- c(cors, cor(x.list[[combos[1, i]]], x.list[[combos[2, i]]])) (r &lt;- mean(cors)) ## [1] 0.804403 From this value \\(r = 0.804\\), we can obtain the path coefficient and the (standardized) error variance: sqrt(r) # path coefficient ## [1] 0.8968852 1 - r # standardized error variance ## [1] 0.195597 (1 - r) * var(unlist(x.list)) # unstandardized error variance ## [1] 0.213149 In summary: the standardized coefficient (the loading) linking indicator to latent variables is the square-root of the reliability. The standardized error variance is 1 - reliability. So far, we have only dealt with latent variables as exogenous (predictor) variables, but they can also act as endogenous (response) variables. Here is an example endogenous latent variable: The graph looks roughly similar, with some changes in the parameters: the error variance on \\(y\\) is now \\(\\epsilon_{y}\\), while the latent variable itself is represented as \\(\\eta\\) and it has its own error \\(\\zeta\\). The presence of this additional error presents a challenge: we simply dont have enough information to estimate all the unknowns here. In this case, we assume no measurement error on \\(y\\) such that \\(\\epsilon_{y} = 0\\). Consequently, \\(y\\) becomes a perfect indicator of \\(\\eta\\) such that the reliability is total and \\(\\lambda_{y} = 1\\). We will get the calculation of \\(\\zeta\\) now because this involves the values of one or more paths leading into the endogenous \\(\\eta\\). 7.2 Application of Latent Variables to Path Models Having now described both exogenous and endogenous latent variables now allows us to fit a structural model, or one with directed paths between latent variables. This is in contrast to a measurement model, which focuses solely on relating indicators to latent variables. As an example of a structural model, lets combine the two latent variable models so that the exogenous latent variable is predicting the endogenous one: As before, lets fix the error of \\(y\\) to be 0 so that the loading on \\(\\eta = 1\\). We can solve the exogenous paths as before, leaving us with two parameters left: the path coefficient \\(\\gamma\\) and \\(\\zeta\\). We can solve the path coefficient \\(\\gamma\\) by knowing the regression coefficient (correlation) between the raw values of \\(x\\) and \\(y\\) and adjusting by the loading of \\(x\\) on \\(\\xi\\). Lets return to our previous example and generate some data for \\(y\\), then estimate the standardized coefficient or correlation: set.seed(3) y &lt;- x + runif(10, 0, 5) y.list &lt;- lapply(1:5, function(i) y + runif(10, 0, 2)) y. &lt;- unlist(y.list) xy_model &lt;- lm(y. ~ x.) beta &lt;- summary(xy_model)$coefficients[2, 1] (beta_std &lt;- beta * (sd(x.) / sd(y.))) # standardized ## [1] 0.5440115 cor(x., y.) # same as the standardized coefficient for simple regression ## [1] 0.5440115 In this example, the estimated standardized path coefficient for \\(x\\) on \\(y\\) is \\(b = 0.544\\). We can obtain an estimate of gamma using the following equation: \\[\\gamma = \\frac{b}{\\lambda_{x}}\\] Which, for our example, is: (gamma &lt;- beta_std / sqrt(r)) ## [1] 0.6065565 So the new estimate of the coefficient between the two latent variables is \\(\\gamma = 0.607\\) which of course is different from \\(b = 0.544\\). This is because the measurement error in \\(x\\) was formerly lumped in to the prediction error of \\(y\\). By removing it, we have improved the estimate of the true effect of \\(x\\) on \\(y\\) and validated our assumption about the error variance of \\(y\\). Not accounting for measurement error, then, results in a downward bias in both the coefficients and the variance explained. From this value, we can obtain the unexplained variance, or \\(\\zeta\\). Recall that the error \\(\\delta_{x}\\) is 1 - the explained variance, where the explained variance is the reliability. Here, we can transfer this knowledge such that: \\(\\zeta = 1 - \\gamma^2\\): 1 - gamma^2 ## [1] 0.6320892 # compare to regression residual variance 1 - summary(xy_model)$r.squared ## [1] 0.7040515 The error variance has decreased from 0.704 to 0.632 relative to the linear model, again, as a consequence of removing the measurement error in \\(x\\) in the predictions of \\(y\\) and improving our estimate of the true relationship between the two. 7.3 Latent Variables in lavaan Lets reproduce this example using lavaan. The setup is almost identical except for a new operator =~ which indicates a latent variable. Additionally, we will fix the error variance in \\(x\\) to the known (unstandardized) error variance from our repeated trials (recall how to fix coefficients from the chapter on multigroup models). library(lavaan) (1 - r) * var(x.) # unstandardized error variance ## [1] 0.213149 latent_formula1 &lt;- &#39; xi =~ x # exogenous latent eta =~ y # endogenous latent eta ~ xi # path model x ~~ 0.213 * x # fix error variance &#39; latent_model1 &lt;- sem(latent_formula1, data.frame(x = x., y = y.)) summary(latent_model1, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 50 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## xi =~ ## x 1.000 0.925 0.895 ## eta =~ ## y 1.000 1.504 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta ~ ## xi 0.989 0.221 4.469 0.000 0.608 0.608 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x 0.213 0.213 0.199 ## .y 0.000 0.000 0.000 ## xi 0.855 0.214 4.003 0.000 1.000 1.000 ## .eta 1.426 0.327 4.363 0.000 0.630 0.630 ## ## R-Square: ## Estimate ## x 0.801 ## y 1.000 ## eta 0.370 If we examine the output, we find a poor-fitting model, but lets ignore that for now considering these were just random data. Instead, lets focus on the estimated parameters and compare them to our hand-calculated values. The standardized loading on \\(xi = 0.895\\) which is very close to the value we calculated \\(\\sqrt(r) = 0.897\\). The loading on \\(\\eta\\) is \\(\\lambda_{y} = 1\\). Notice how we didnt specify that: the default in lavaan is to set the first loading to 1 when the error variance is not supplied (more on this later). With respect to the regression coefficient, lavaan returned a standardized \\(\\gamma = 0.608\\) while we obtained \\(\\gamma = 0.607\\). Very close! Similarly the standardized error variance on \\(\\eta\\) is \\(\\zeta = 0.630\\), which is also very close to \\(1 - \\gamma^2 = 0.632\\). Naturally, then, the explained variances are also nearly identical, being 1 - error variance. So, all in all, for single indicator latent variables, we are able to almost exactly reproduce the output from lavaan (slight deviations are due to the optimization algorithms to solve the maximum-likelihood fitting function). One could alternately fix the error of the exogenous latent variable and incorporate measurement error of \\(y\\): cors.y &lt;- c() for(i in 1:ncol(combos)) cors.y &lt;- c(cors.y, cor(y.list[[combos[1, i]]], y.list[[combos[2, i]]])) (r.y &lt;- mean(cors.y)) ## [1] 0.8535083 (1 - r.y) * var(y.) # unstandardized error variance ## [1] 0.3380926 latent_formula2 &lt;- &#39; xi =~ x # exogenous latent eta =~ y # endogenous latent eta ~ xi # path model y ~~ 0.338 * y # fix error variance &#39; latent_model2 &lt;- sem(latent_formula2, data.frame(x = x., y = y.)) summary(latent_model2, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 50 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## xi =~ ## x 1.000 1.033 1.000 ## eta =~ ## y 1.000 1.387 0.922 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta ~ ## xi 0.792 0.173 4.584 0.000 0.590 0.590 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y 0.338 0.338 0.149 ## .x 0.000 0.000 0.000 ## xi 1.068 0.214 5.000 0.000 1.000 1.000 ## .eta 1.254 0.318 3.939 0.000 0.652 0.652 ## ## R-Square: ## Estimate ## y 0.851 ## x 1.000 ## eta 0.348 Here, because we did not specify it, the error variance of \\(x\\) has automatically been fixed to 0 and the loading to 1, again, because we have more unknowns than knowns and have to sacrifice a value to get estimates for everything else. To start, the standardized \\(\\gamma = 0.590\\), which is different than the \\(\\gamma = 0.608\\) we obtained when incorporating measurement error on \\(x\\), but also different than the standardized coefficient from a simple linear regression \\(b = 0.544\\). The error variance on \\(\\eta\\) (\\(\\zeta = 0.652\\)) is also lower than the unexplained variance from the linear regression (\\(1 - R^2 = 0.704\\)), but higher than in the latent variable model incorporating error on \\(x\\) (\\(\\zeta = 0.630\\)). Thus, we see that incorporating measurement error in endogenous latent variables resolves some of the downward bias in the unstandardized coefficient and error variance. For the moment, latent variables are restricted to covariance-based SEM, although new work by Shipley and Douma have extended latent variable modeling to a graph theoretic approachk. For the time being, however, lavaan provides an easier, robust framework that easily extends to multi-indicator latent variables, and so we will use it from here on out. 7.4 Multi-indicator Latent Variables Accounting for measurement error requires some estimate of reliability. Often, we dont have a measure of reliability, because we dont design our experiments to repeatedly sample the indicator variable to obtain one. In such cases, it might be recommended to revert to a non-latent variable approach where the path coefficients are simlpy regression coefficients that dont incorporate any measurement error. Another solution is to incorporate multiple indicator variables to provide a different measure of reliability. In this case, the correlation is not derived from multiple samples of the same indicator, but among indicators. It also acts as a check against indicators that do not inform the latent variable, as such variables will provide low reliability estimates because they are (presumably) not being generated by that same underlying latent variable. This approach also provides a conceptual advantage: we often choose a single indicator as a surrogate for a latent concept (e.g., body mass for body size). Including more indicators helps to generalize this phenomenon by testing that the result is not an impact of the choice of any single indicator. Multiple indicators raises a new problem, though: identifiability. Remember from the chapter on global estimation that we must have enough known pieces of information to estimate all the unknown quantities implied by the model. Latent variable models must also follow the t-rule. Consider an exogenous latent variable indicated by two variables, \\(x1\\) and \\(x2\\). We can break this latent variable into two equations: \\[x1 = \\lambda_{1}\\xi + \\delta_{x1}\\] \\[x2 = \\lambda_{2}\\xi + \\delta_{x2}\\] We know the values of \\(x1\\) and \\(x2\\) and the correlation between them. To estimate values for the latent construct \\(\\xi\\) we need to estimate \\(\\lambda_{1}\\), \\(\\lambda_{2}\\), \\(\\delta_{x1}\\) and \\(\\delta_{x2}\\). This model fails the t-rule, which, if you recall, is: \\[t \\leq \\frac{n(n+1)}{2}\\] where \\(t = 4\\) is the number of unknowns, and \\(n = 2\\) is the number of knowns. In this example, \\(t = 4 \\leq 3\\) does not hold. Since \\(\\delta = 1 - \\lambda^2\\), we need only solve for the two \\(\\lambda\\)s, but we only have one piece of information: the correlation. The solution is to set the loadings to be equal: \\(\\lambda_{1} = \\lambda_{2}\\). This is because, with only this information, we have no reason to suspect one indicator is more correlated with the latent variable than the other. Its important to note here that the two must be positively correlated (or scaled to be so), otherwise setting them to be equal is not a valid assumption. We know from our ninth rule of path coefficients that the correlation equals the sum of the direct and indirect pathways. The only path connecting \\(x1\\) and \\(x2\\) is through \\(\\epsilon\\), and the value of the compound path is the product of the two individual pathways (Rule 3). Thus, the correlation \\(r_{x1,x2} = \\lambda_{1} \\times \\lambda_{2}\\). Given the assumption that the two loadings are equal, \\(r_{x1,x2} = \\lambda^2\\) and thus \\(\\lambda = \\sqrt(r_{x1,x2})\\). We can scale this procedure for &gt;2 indicators by setting just the two loadings to be equal: this will give us the necessary information (along with Rule 8 of path coefficients) to generate unique solutions for the other loadings. It is for this reason that at least three indicators are preferred for multi-indicator latent variables: it lessens the impact of the assumption that two of the loadings are equal. A potentially betterand easiersolution is to fix one of the loadings to be 1. If, for example, we fix \\(\\lambda_{1} = 1\\) then we know that \\(\\lambda_{2} = r_{x1,x2} / \\lambda_{1} = r_{x1,x2} / 1 = r_{x1,x2}\\). This choice has another consequence: because it is unmeasured, we also need to provide a scale for our multi-indicator latent variable. This can be done by fixing the variance \\(\\zeta = 1\\) or by fixing one of the unstandardized loadings to 1 (wherein the scale will be in units of that indicator). Both accomplish the same objective. Finally, we can obtain an integrated estimate of reliability from multi-indicator latent variables using the following equation: \\[\\rho = \\frac{(\\sum\\lambda_{j})^2}{(\\sum\\lambda_{j})^2 + \\sum\\epsilon_{j}} \\] where \\(j\\) is the index of each indicator variable. For the record, a reliability index &gt; 0.9 is considered excellent, &gt; 0.8 to be good, and so on. Anything &lt; 0.5 is considered to be no different than random chance, and so indicators with such a low degree of correlation should be avoided. In fact, it is always recommended to inspect the correlation matrix among indicator variables to screen for potentially unrelated indicators. It may also help to identify indicators that are highly correlated, moreso than the other indicators. Such high correlations might suggest another common cause (such as the same measurement instrument, same observer, evolutionary constraints, etc.). In this case, it would be recommended to indicate a correlated error among the two indicators indicating a separate underlying driver of their higher-than-average association than the specified latent construct. When we get into the realm of multi-indicator latent variables, it becomes impossible to decompose partial relationships as we have previously for observed variable models. Instead, maximum-likelihood functions are necessary to iteratively test and optimize the parameters that describe the relationships between observed and unobserved quantities. As in observed-variable models, the maximum-likelihood fitting function (\\(F_{ML}\\)) can be used to construct a \\(\\chi^2\\) statistic that is the difference between the observed and model-implied variance-covariance matrices. In the case of latent variable models, the covariances among latent variables as well as the loadings are considered in constructing the estimated covariance matrix. Beyond that, the procedure is the same as for observed variable models in terms of calculating \\(\\chi^2\\) and testing it against the \\(\\chi^2\\)-distribution with some model degrees of freedom. 7.5 Confirmatory Factor Analysis Multi-indicator latent variables can also be used to the test the hypothesis that a suite of indicator variables are generated by the same underlying process. This is also called confirmatory factor analysis. In other words, you are testing the idea that the latent variable has given rise to emergent properties that, by virtue of a common cause, are correlated. This approach concerns only the measurement model and thus is a precursor to evaluation of any structural models in which the latent variables appear. In contrast, exploratory factor analysis assumes that all latent variables are indicated by all observed variables. [content pending] 7.6 Travis &amp; Grace (2010): An Example Lets apply these concepts to an example dataset from Travis &amp; Grace (2010). In this example, the authors transplanted individuals of the salt marsh plant Spartina alterniflora and measured their performance relative to local populations. In this case, performance was captured by a number of variables including: stem density, the number of infloresences, clone diameter, leaf height, and leaf width. The difference between transplants and local individuals was quantified using their genetic dissimilarity. In this case, the authors considered performance to be a latent construct that manifests in the five indicators listed above: Lets first explore this latent construct before getting into the structural model. First, lets examine the raw correlations to see if this construct is justifiable: travis &lt;- read.csv(&quot;https://raw.githubusercontent.com/jslefche/sem_book/master/data/travis.csv&quot;) cor(travis[, 4:8]) ## stems infls clonediam leafht leafwdth ## stems 1.0000000 0.8339227 0.9333150 0.7275625 0.6457378 ## infls 0.8339227 1.0000000 0.8126388 0.6925888 0.6026302 ## clonediam 0.9333150 0.8126388 1.0000000 0.7729843 0.7296621 ## leafht 0.7275625 0.6925888 0.7729843 1.0000000 0.9687725 ## leafwdth 0.6457378 0.6026302 0.7296621 0.9687725 1.0000000 The correlations range from 0.65-0.93, suggesting that many of these variables may be generated by the same process. There is one excessive correlation between leaf height and width, potentially suggesting influence by another process (hint hint). Now that we have qualitatively assessed the validity of the latent model, lets fit it and examine the output: travis_latent_formula1 &lt;- &#39;performance =~ stems + infls + clonediam + leafht + leafwdth&#39; travis_latent_model1 &lt;- sem(travis_latent_formula1, travis) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov ## variances are negative summary(travis_latent_model1) ## lavaan 0.6-9 ended normally after 82 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 23 ## ## Model Test User Model: ## ## Test statistic 51.106 ## Degrees of freedom 5 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## performance =~ ## stems 1.000 ## infls 0.126 0.037 3.377 0.001 ## clonediam 1.160 0.309 3.751 0.000 ## leafht 1.215 0.244 4.971 0.000 ## leafwdth 0.151 0.031 4.822 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .stems 125.886 37.014 3.401 0.001 ## .infls 2.405 0.707 3.403 0.001 ## .clonediam 132.478 39.038 3.394 0.001 ## .leafht -1.847 5.336 -0.346 0.729 ## .leafwdth 0.223 0.105 2.131 0.033 ## performance 135.763 67.580 2.009 0.045 Note that the first loading has been restricted to 1 (the default in lavaan) for purposes of identifiability. First, we note that the model is a poor fit (P &lt; 0.001). We can explore why this is using modification indices: print(modindices(travis_latent_model1)) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 12 stems ~~ infls 10.470 11.784 11.784 0.677 0.677 ## 13 stems ~~ clonediam 17.152 112.521 112.521 0.871 0.871 ## 14 stems ~~ leafht 0.693 -7.889 -7.889 -0.517 -0.517 ## 15 stems ~~ leafwdth 2.214 -1.836 -1.836 -0.346 -0.346 ## 16 infls ~~ clonediam 8.773 11.092 11.092 0.621 0.621 ## 17 infls ~~ leafht 0.062 -0.312 -0.312 -0.148 -0.148 ## 18 infls ~~ leafwdth 2.906 -0.281 -0.281 -0.383 -0.383 ## 19 clonediam ~~ leafht 4.028 -21.233 -21.233 -1.357 -1.357 ## 20 clonediam ~~ leafwdth 0.037 -0.261 -0.261 -0.048 -0.048 ## 21 leafht ~~ leafwdth 37.862 17.177 17.177 26.752 26.752 Recall that the value of the modification index (mi in the output) is the expected decrease in the model \\(\\chi^2\\). Here, a larger number would lead to a better fit if that path were included. It seems there is a strong implied correlation between leaf height and leaf width, presumably arising from common constraints on how the leaves of Spartina have evolved and the limited variety of shapes they can take, and not from the plants performance. We can introduce this correlation into the model and re-fit: travis_latent_formula2 &lt;- &#39; performance =~ stems + infls + clonediam + leafht + leafwdth leafht ~~ leafwdth &#39; travis_latent_model2 &lt;- sem(travis_latent_formula2, travis) summary(travis_latent_model2) ## lavaan 0.6-9 ended normally after 81 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 11 ## ## Number of observations 23 ## ## Model Test User Model: ## ## Test statistic 7.410 ## Degrees of freedom 4 ## P-value (Chi-square) 0.116 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## performance =~ ## stems 1.000 ## infls 0.117 0.016 7.173 0.000 ## clonediam 1.086 0.096 11.319 0.000 ## leafht 0.697 0.127 5.509 0.000 ## leafwdth 0.082 0.018 4.529 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .leafht ~~ ## .leafwdth 10.831 3.432 3.156 0.002 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .stems 15.267 10.877 1.404 0.160 ## .infls 1.204 0.390 3.085 0.002 ## .clonediam 24.786 13.830 1.792 0.073 ## .leafht 78.958 24.465 3.227 0.001 ## .leafwdth 1.672 0.509 3.283 0.001 ## performance 246.382 77.658 3.173 0.002 Ah-ha! Introducing this correlated error has now reduced the \\(\\chi^2\\) statistic to an acceptably low level to achieve an adequate model fit (P = 0.116). Thus, we fail to reject our latent construct of plant performance, which we can now use to evaluate some broader hypotheses. If you recall, the authors original intent was to explore how native vs. non-native genotypes of Spartina influenced performance, which they quantified using a measure of genetic distance from the local population. To test this hypothesis, lets fit the following path model: Lets fit the above model: travis_path_formula1 &lt;- &#39; # latent performance =~ stems + infls + clonediam + leafht + leafwdth # structural paths performance ~ geneticdist # correlated errors leafht ~~ leafwdth &#39; travis_path_model1 &lt;- sem(travis_path_formula1, travis) summary(travis_path_model1) ## lavaan 0.6-9 ended normally after 107 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 23 ## ## Model Test User Model: ## ## Test statistic 12.237 ## Degrees of freedom 8 ## P-value (Chi-square) 0.141 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## performance =~ ## stems 1.000 ## infls 0.117 0.017 6.929 0.000 ## clonediam 1.106 0.096 11.508 0.000 ## leafht 0.711 0.127 5.601 0.000 ## leafwdth 0.084 0.018 4.650 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## performance ~ ## geneticdist -51.673 11.365 -4.547 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .leafht ~~ ## .leafwdth 10.416 3.312 3.145 0.002 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .stems 19.691 10.733 1.835 0.067 ## .infls 1.246 0.401 3.108 0.002 ## .clonediam 19.411 12.364 1.570 0.116 ## .leafht 76.177 23.645 3.222 0.001 ## .leafwdth 1.612 0.492 3.278 0.001 ## .performance 120.509 39.656 3.039 0.002 It seems that this model fits the data well (P = 0.141) and the relationship of interest (between \\(geneticdist\\) and \\(performance\\)) is highly significant (P &lt; 0.001). In this case, the more distant the genotypes of the local population from which the transplants were taken (i.e., a greater genetic distance), the worse they performed. 7.7 References Travis, S. E., &amp; Grace, J. B. (2010). Predicting performance for ecological restoration: a case study using Spartina alterniflora. Ecological Applications, 20(1), 192-204. "],["composite-variables.html", "8 Composite Variables 8.1 What is a Composite Variable? 8.2 Constructing a Composite Variable 8.3 Grace &amp; Keeley Revisited: A Worked Example 8.4 Composites in piecewiseSEM 8.5 References", " 8 Composite Variables 8.1 What is a Composite Variable? Composite variables are another way besides latent variables to represent complex multivariate concepts in structural equation modeling. The most important distinction between the two is that, while latent variables give rise to measurable manifestations of an unobservable concept, composite variables arise from the total combined influence of measured variables. Consider the following composite variable \\(\\eta\\): Here, the arrows are leading into, not out of, \\(\\eta\\), indicating that the composite variable is made up of the influences of the three observed variables. Note: in this and other presentations, the composite is denoted by a hexagon, but can sometimes be an oval as it can technically be a form of a latent variable. If the composite is entirely made up of the three influences, it can be said to have no error. An example might be the two levels of a treatment that lead into a single composite variable called Treatment. In this case, there are no other levels of treatment because you, as the investigator, did not apply any. Thus the composite treatment captures the full universe of treatment possibilities given the data. In other cases, the property might arise from the collective influence of variables but is not without error. For example, the idea of soil condition arises from different aspects of the soil: its pH, moisture, grain size, and so on. However, one might measure only some of these, and thus there remain other factors (nutrient content, etc.) that might contribute to the notion of soil condition. In this case, the composite would have error and is therefore known as a latent composite. The benefit of such an approach is that complicated constructs can be distilled into discrete blocks that are easier to present and discuss. For example, it is easier to talk about the effects of the experimental treatment on soil condition, rather than the effect of treatment 1 on soil moisture, the effect of treatment 1 on soil pH, the effect of treatment 1 on soil grain size, and so on. In this way, the composite harkens back to the early meta-model, or broad conceptual relationships that inform the parameterization of the structural path model. In fact, in populating the meta-model, you may wish to consider those broad concepts as composites (or latents) when fitting the model, rather than modeling all relationships among all observed variables. Selecting between latent and composite variables comes down to the concept in question, the presumed direction of causality, and the nature of the indicators. For the soil example, consider: is it that there is a common difference among soils driving variation in pH, moisture, etc.? Or is it that pH, moisture, etc. are all independent properties that combine to inform soil condition? If the goal is measure plant growth in potting soils from different manufacturers, then manufacturer might be the common source of variation and a latent variable more appropriate. If the observer is visiting different sites and measuring conditions that describe the soil in each place, then perhaps a composite variable is warranted. Another way of thinking about this is whether the indicators are interchangeable. In other words, does soil pH tell us the same information as soil moisture? If so, then they might be indicators of the same latent phenomenon. If not, and they contain unique information, then they likely combine to form a composite variable. Finally, do the indicators co-vary? If they are under common control of a latent variable, then changing one should alter all the others. If they are relatively independentfor example, one could change grain size without changing nutrient contentthen causation likely flows into a composite (rather than out of a latent) variable. Now that we have defined a composite variable, lets see how to make one. 8.2 Constructing a Composite Variable Compared to latent variables, a composite variable is actually very easy to estimate: it is simply the sum of its indicators, hence the term composite. The way in which the indicators are summed depends on whether they are expected to have the same weight (a fixed composite) or different weights (a statistical composite). The former might be something like species relative abundances. The latter is what we will focus on here because it has the most practical applications in ecology. The weights for the composite are easily acquired as they are the values that maximize the explained variance in some response. We have done this before many times using maximum-likelihood fitting. In fact, statistical composites can be boiled down to the coefficients from a multiple regression: The maximum-likelihood fitting function chooses parameter estimates for each predictor that maximize the likelihood of observing the response; Those parameter values serve as the loadings for the indicators of the composite variable; The data for each indicator are multiplied by their loading and summed to generate the factor scores for the composite variable, which is then used in the structural model. Lets demonstrate using some random data: set.seed(8) y &lt;- rnorm(50) x1 &lt;- rnorm(50) x2 &lt;- x1 + runif(50) # run multiple regression model &lt;- lm(y ~ x1 + x2) # get loadings beta_x1 &lt;- summary(model)$coefficients[2, 1] beta_x2 &lt;- summary(model)$coefficients[3, 1] # compute factor scores composite &lt;- beta_x1 * x1 + beta_x2 * x2 These summed values can then used to predict the response: summary(lm(y ~ composite)) ## ## Call: ## lm(formula = y ~ composite) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.88387 -0.70164 0.05644 0.54393 2.07769 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.3031 0.2424 -1.250 0.217 ## composite 1.0000 0.8045 1.243 0.220 ## ## Residual standard error: 1.014 on 48 degrees of freedom ## Multiple R-squared: 0.03118, Adjusted R-squared: 0.011 ## F-statistic: 1.545 on 1 and 48 DF, p-value: 0.2199 Note how the unstandardized coefficient is 1. This is because the composite is in units of the predicted values of the response. Thus, the coefficient is really only interpretable in standardized units. Lets alternately fit this composite model with lavaan and fix the loadings of \\(x1\\) and \\(x2\\) to the values from the multiple regression: library(lavaan) comp_formula1 &lt;- &#39; composite &lt;~ -0.498 * x1 + 0.579 * x2 y ~ composite &#39; comp_model1 &lt;- sem(comp_formula1, data.frame(y, x1, x2)) summary(comp_model1, standardize = T) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 50 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 1 ## P-value (Chi-square) 0.999 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Composites: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## composite &lt;~ ## x1 -0.498 -2.793 -3.163 ## x2 0.579 3.248 3.642 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## y ~ ## composite 1.000 0.788 1.269 0.205 0.178 0.177 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y 0.987 0.197 5.000 0.000 0.987 0.969 ## composite 0.000 0.000 0.000 We see from the output that the estimated loadings for our two indicators are the same values we provided, and consequently the understandardized coefficient is 1. However, the standardized coefficient is 0.177 and it is this value that we would present (although its non-significant, given that these are fake data). Lets suppose we didnt know the loadings from the multiple regression. We run into the same issue of identifiability as when constructing latent variables, so we must fix the first loading to 1. This will also define the scale of the composite. NOTE: lavaan does not do this automatically (as it does for latents), so we will have to implement it manually. comp_formula2 &lt;- &#39; composite &lt;~ 1 * x1 + x2 y ~ composite &#39; comp_model2 &lt;- sem(comp_formula2, data.frame(y, x1, x2)) ## Warning in lavaan::lavaan(model = comp_formula2, data = data.frame(y, x1, : lavaan WARNING: ## the optimizer warns that a solution has NOT been found! It seems that, because the true loading of \\(x1\\) on the composite is far from 1 (we know it is actually -0.498), we have received a non-convergence error! One solution is to set the other loading to 1: comp_formula3 &lt;- &#39; composite &lt;~ x1 + 1 * x2 y ~ composite &#39; comp_model3 &lt;- sem(comp_formula3, data.frame(y, x1, x2)) summary(comp_model3, standardize = T) ## lavaan 0.6-9 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 50 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Composites: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## composite &lt;~ ## x1 -0.860 0.230 -3.745 0.000 -2.792 -3.162 ## x2 1.000 3.247 3.642 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## y ~ ## composite 0.579 0.489 1.184 0.236 0.178 0.177 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y 0.987 0.197 5.000 0.000 0.987 0.969 ## composite 0.000 0.000 0.000 Here the model converges because the true loading (0.579) is close enough to 1 for the maximum-likelihood fitting function to find it within a certain number of iterations. Note that the unstandardized coefficient is no longer 1: this is because the scale of the composite has been set to that of the second indicator. For reasons of model convergence, it is generally recommended that one compute the loadings by hand and fix them in the model. This has an added benefit we will get to in a later section. But first lets explore a real-world example. 8.3 Grace &amp; Keeley Revisited: A Worked Example Recall from the chapters on global and local estimation that Grace &amp; Keeley (2006) were interested in the factors that mediated recovery of shrublands post-fire disturbance. In those chapters, we fit different sub-models of their larger model, and well fit a different sub-model yet again in this chapter for simplicity. In their model, they used plant cover to predict plant species richness. Lets assume for a moment that the relationship between cover and richness may be non-linear: its not until a certain amount of cover that rarer species begin to appear, for example. In this case, we might suppose there are both linear \\(cover\\) and non-linear \\(cover^2\\) components to the model. Composite variables are a nice way to summarize both the linear and non-linear effects. Lets fit the following composite variable: Here we have a composite that summarizes the unsquared and squared values of cover, which then goes on to predict richness. Lets adopt the two-step approach and first fit a linear model. library(piecewiseSEM) data(keeley) cover_model &lt;- lm(rich ~ cover + I(cover^2), keeley) summary(cover_model) ## ## Call: ## lm(formula = rich ~ cover + I(cover^2), data = keeley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.161 -11.958 -0.595 10.094 32.956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.641 6.574 3.900 0.000189 *** ## cover 57.999 18.931 3.064 0.002910 ** ## I(cover^2) -28.577 12.385 -2.307 0.023403 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14 on 87 degrees of freedom ## Multiple R-squared: 0.1598, Adjusted R-squared: 0.1404 ## F-statistic: 8.271 on 2 and 87 DF, p-value: 0.0005147 It seems both the unsquared and squared values of cover significantly predict richness, so we are justified in including both as indicators to our composite variable. Now we extract the coefficients, use them to generate the factor scores, and finally use those scores to predict richness. beta_cover &lt;- summary(cover_model)$coefficients[2, 1] beta_cover2 &lt;- summary(cover_model)$coefficients[3, 1] composite &lt;- beta_cover * keeley$cover + beta_cover2 * (keeley$cover)^2 summary(lm(rich ~ composite, data = data.frame(keeley, composite))) ## ## Call: ## lm(formula = rich ~ composite, data = data.frame(keeley, composite)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.161 -11.958 -0.595 10.094 32.956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.6406 5.9516 4.308 4.27e-05 *** ## composite 1.0000 0.2445 4.090 9.51e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.93 on 88 degrees of freedom ## Multiple R-squared: 0.1598, Adjusted R-squared: 0.1502 ## F-statistic: 16.73 on 1 and 88 DF, p-value: 9.507e-05 As would be expected from the multiple regression, the composite term significantly predicts richness (P &lt; 0.001). Lets use the coefs function from piecewiseSEM to obtain the standardized coefficient: coefs(lm(rich ~ composite, data = data.frame(keeley, composite))) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 rich composite 1 0.2445 88 4.0904 1e-04 0.3997 *** So a 1 standard deviation change in the total cover effect would result in a 0.40 standard deviation change in plant richness. We can alternately fit the model with lavaan using the same coefficients from the multiple regression: # create a new non-linear variable for cover^2 keeley$coversq &lt;- keeley$cover^2 keeley_formula1 &lt;- &#39; composite &lt;~ 58 * cover + -28.578 * coversq rich ~ composite &#39; keeley_model1 &lt;- sem(keeley_formula1, keeley, fixed.x = F) summary(keeley_model1, standardize = T) ## lavaan 0.6-9 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 1 ## P-value (Chi-square) 1.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Composites: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## composite &lt;~ ## cover 58.000 9.660 3.048 ## coversq -28.578 -4.760 -2.295 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## rich ~ ## composite 1.000 0.242 4.137 0.000 6.004 0.400 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## cover ~~ ## coversq 0.147 0.022 6.602 0.000 0.147 0.969 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .rich 189.597 28.263 6.708 0.000 189.597 0.840 ## composite 0.000 0.000 0.000 ## cover 0.100 0.015 6.708 0.000 0.100 1.000 ## coversq 0.233 0.035 6.708 0.000 0.233 1.000 Which leads us to the same standardized coefficient (0.40) as through the manual calculation. Finally, lets incorporate the effect of fire severity on cover and richness. Now the composite is endogenous because it is affected by fire severity, and goes on to predict richness. First, we must fit the model without the composite to get the loadings of \\(cover\\) and \\(cover^2\\). keeley_formula2 &lt;- &#39; composite &lt;~ 58 * cover + -28.578 * coversq rich ~ composite + firesev cover ~ firesev cover ~~ coversq firesev ~~ coversq &#39; keeley_model2 &lt;- sem(keeley_formula2, keeley) summary(keeley_model2, standardize = T, rsq = T) ## lavaan 0.6-9 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.065 ## Degrees of freedom 1 ## P-value (Chi-square) 0.799 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Composites: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## composite &lt;~ ## cover 58.000 9.660 3.048 ## coversq -28.578 -4.760 -2.295 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## rich ~ ## composite 0.731 0.265 2.757 0.006 4.391 0.292 ## firesev -2.132 0.969 -2.200 0.028 -2.132 -0.233 ## cover ~ ## firesev -0.084 0.018 -4.611 0.000 -0.084 -0.437 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cover ~~ ## coversq 0.122 0.019 6.588 0.000 0.122 0.893 ## coversq ~~ ## firesev -0.301 0.089 -3.369 0.001 -0.301 -0.380 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .rich 179.922 26.821 6.708 0.000 179.922 0.797 ## .cover 0.081 0.012 6.708 0.000 0.081 0.809 ## coversq 0.233 0.035 6.708 0.000 0.233 1.000 ## firesev 2.700 0.402 6.708 0.000 2.700 1.000 ## composite 0.000 0.000 0.000 ## ## R-Square: ## Estimate ## rich 0.203 ## cover 0.191 Note that the squared and unsquared terms for \\(cover\\) have correlated errors, because they are both driven by the underlying values of cover. Also, because the squared term is not a true variable in the model but a convenience for us to explore this non-linearity, we must treat \\(cover\\) and \\(cover^2\\) as exogenous even though they are part of an endogenous composite. lavaan automatically models correlations among exogenous variables, but will not do so for the composite indicators unless told explicitly. In this case, then, we must manually control for the correlation between \\(cover^2\\) and \\(cover\\) and \\(firesev\\). Here, we find a good-fitting model (P = 0.80). Moreover, we obtain the standardized coefficient for the effect of fire severity on cover \\(\\gamma = -0.437\\) and of the composite on richness \\(\\beta = 0.292\\) controlling for fire severity, which is the total non-linear effect of cover. To otain the indirect effect then, we multiply these paths plus the standardized loading \\(cover\\) on the composite: \\(-0.437 * 3.048 * 0.292 = -0.389\\). 8.4 Composites in piecewiseSEM For the moment, composites are not directly implemented in piecewiseSEM with special syntax like in lavaan, but we hope to introduce that functionality soon. In the interim, they are easy to compute them by hand, as we have shown above, extract the predicted scores, and use them as any other predictor. Lets examine this with the Keeley model as above: keeley$composite &lt;- composite keeley_psem &lt;- psem( lm(cover ~ firesev, keeley), lm(rich ~ composite + firesev, keeley) ) summary(keeley_psem, .progressBar = FALSE) ## ## Structural Equation Model of keeley_psem ## ## Call: ## cover ~ firesev ## rich ~ composite + firesev ## ## AIC ## 765.393 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## cover ~ composite + ... coef 87 11.6011 0.0000 *** ## rich ~ cover + ... coef 86 -0.2497 0.8034 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 84.206 with P-value = 0 and on 2 degrees of freedom ## Fisher&#39;s C = 86.213 with P-value = 0 and on 4 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## cover firesev -0.0839 0.0184 88 -4.5594 0.0000 -0.4371 *** ## rich composite 0.7314 0.2698 87 2.7108 0.0081 0.2923 ** ## rich firesev -2.1323 0.9859 87 -2.1629 0.0333 -0.2332 * ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## cover none 0.19 ## rich none 0.20 Note that we get the same standardized coefficients as in lavaan! There is, however, deviation in the goodness-of-fit that are accounted for by differences in how the composite is constructed and the correlated errors among the indicator and \\(firesev\\) which are not possible to model in piecewiseSEM yet. 8.5 References Grace, J. B., &amp; Keeley, J. E. (2006). A structural equation model analysis of postfire plant diversity in California shrublands. Ecological Applications, 16(2), 503-514. "]]
